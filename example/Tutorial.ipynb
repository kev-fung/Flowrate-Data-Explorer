{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlflowrate: Tutorial\n",
    "\n",
    "This notebook will loosely guide users on how to use the data integration part of the software on a local jupyter notebook environment.\n",
    "\n",
    "However, the same methods/logic flow are also applied for an Azure Databricks notebook.\n",
    "\n",
    "To import the software, one must call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflowrate.workflow import WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import the relevant Spark frameworks, within an Azure Databricks notebook this will not be necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import lit, unix_timestamp\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we construct some dummy datasets for processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy1 = [(\"JJ\", 1.0, 2, 3, 8, 5), (\"Bizarre\", 1.8, 3, 3, 5, None)]\n",
    "df1 = spark.createDataFrame(dummy1, [\"datetime\", \"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "\n",
    "dummy2 = [(\"John\", 1.0, 2, 3, 4, 5), (\"Snow\", 1.3, None, 4, 5, 6), (\"JJ\", 1.0, 2, 3, 8, 5), (\"Bizarre\", 1.8, 3, 3, 5, 6)]\n",
    "df2 = spark.createDataFrame(dummy2, [\"datetime\", \"a\", \"b\", \"c\", \"d\", \"e\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start using the package, we must instantiate the data management module WorkFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\"d1\":df1, \n",
    "       \"d2\":df2}\n",
    "\n",
    "flow = WorkFlow(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets merge the two pieces of dummy data into a single dataframe called \"df\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.integrate.merge_data(newname=\"df\", first=\"d1\", second=\"d2\", axis=0)\n",
    "flow.integrate.status(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now integrated csv data into the program!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Life Example:\n",
    "\n",
    "For better intuition on how to use the program. The below code shows how the software was used for applying different machine learning models to the data in an Azure Databricks Notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load csv data as Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datas into notebook\n",
    "df_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\n",
    "df_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\n",
    "df_OW1ql = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\n",
    "df_records = spark.read.format('csv').options(header='true', inferSchema='true', delimiter='|', encoding='iso-8859-1').load('/FileStore/tables/interences_filtered.csv')\n",
    "test_OW1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/OW1test_1.csv')\n",
    "\n",
    "# Convert date columns to datetime formats\n",
    "# Rename columns so that it fits with flow package.\n",
    "df_01 = df_01.select(\n",
    "      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n",
    "      df_01[\"name\"].alias(\"tag\"),\n",
    "      df_01[\"value\"])\n",
    "\n",
    "df_02 = df_02.select(\n",
    "      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n",
    "      df_02[\"name\"].alias(\"tag\"),\n",
    "      df_02[\"value\"])\n",
    "\n",
    "df_OW1ql = df_OW1ql.select(\n",
    "      F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "      *[feat for feat in df_OW1ql.columns if feat not in [\"DATE\"]])\n",
    "\n",
    "test_OW1 = test_OW1.select(\n",
    "      F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy HH:mm\").alias(\"datetime\"),\n",
    "      *[n for n in test_OW1.schema.names if not n == \"DATE\"])\n",
    "\n",
    "df_records = df_records.select(\n",
    "      F.to_timestamp(F.col(\"Date\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "      *[n for n in df_records.schema.names if not n == \"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean and Integrate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary for sorting tagname data into oilwells:\n",
    "do = {\"OW1\" : [\"EXAMPLE\", \"EXAMPLE\"],\n",
    "      \"OW2\" : [\"EXAMPLE\", \"EXAMPLE\"],\n",
    "      \"OW3\" : [\"EXAMPLE\"]}\n",
    "\n",
    "#Create dictionary for sorting tagname data into measurements:\n",
    "nn = {\"EXAMPLE\" : \"WHP\",\n",
    "      \"EXAMPLE\" : \"WHT\",\n",
    "      \"EXAMPLE\" : \"GLR\",\n",
    "      \"EXAMPLE\" : \"GLP\",\n",
    "      \"EXAMPLE\" : \"DHP\",\n",
    "      \"EXAMPLE\" : \"DHT\",\n",
    "      \"EXAMPLE\" : \"Choke\",\n",
    "      \"EXAMPLE\" : \"ASD\"}\n",
    "\n",
    "# Collect up data into a dictionary with corresponding names\n",
    "data = {\"df_01\":df_01, \"df_02\":df_02, \"df_OW1ql\":df_OW1ql, \"test_OW1\":test_OW1, \"df_records\":df_records}\n",
    "\n",
    "# Make new workflow object by passing in the data dictionary \n",
    "flow = WorkFlow(data)\n",
    "\n",
    "# Cache data to speed up spark queries\n",
    "flow.integrate.cache_data(\"df_01\", \"df_02\", \"df_OW1ql\", \"test_OW1\", \"df_records\")\n",
    "\n",
    "# Merge dataframes vertically\n",
    "flow.integrate.merge_data(\"df_dump\", \"df_01\", \"df_02\", axis=0)\n",
    "\n",
    "# select the stored data, and remove nulls\n",
    "flow.integrate.clean_data(\"df_dump\", remove_nulls=True)\n",
    "\n",
    "# select the stored data, and reorganise it to more dictionaries\n",
    "flow.integrate.organise_data(\"df_dump\", \"date_tag_val_col\", distinct_oilwells=do, change_sensor_names=nn) \n",
    "\n",
    "# We get three new datasets OW1. OW2. OW3. but their dataframes will be out of shape\n",
    "flow.integrate.clean_data(\"OW1\", avg_over=\"day\", is_dict=True)  # average the data on the dict format\n",
    "flow.integrate.organise_data(\"OW1\", \"dict_col\")  # reorganise the data again so the dictionary matches the dataframe\n",
    "flow.integrate.set_organised(\"OW1\") # Tell the class that this data is ready for dataset assembly\n",
    "flow.integrate.cache_data(\"OW1\") # Cache data frame for fast querying\n",
    "flow.integrate.status(\"OW1\") # check the status of the data\n",
    "\n",
    "flow.integrate.clean_data(\"df_OW1ql\", remove_nulls=True) # remove nulls in the data\n",
    "flow.integrate.edit_col(\"df_OW1ql\", \"Daily liquid rate [Sm3/d]\", newname=\"Qliq\") # edit the column files to have a new name\n",
    "flow.integrate.select_col(\"df_OW1ql\", \"Qliq\") # reselect the data frame to only be the given sample\n",
    "flow.integrate.organise_data(\"df_OW1ql\", \"mult_col\") # reorganise data so that it we have a correponding group dict format\n",
    "flow.integrate.set_organised(\"df_OW1ql\") # Tell the class that this data is ready for assembly\n",
    "flow.integrate.cache_data(\"df_OW1ql\") # cache data frame for fast querying\n",
    "flow.integrate.status(\"df_OW1ql\") # check the status of the data\n",
    "\n",
    "flow.integrate.drop_col(\"test_OW1\", \"WELLNAME\", \"NUMBER\", \"Qo\", \"Qg\", \"Qw\", \"GOR\", \"WOR\", \"WTC\", \"PI\", \"Pres\", \"DCP\", \"ChokeD\", \"DCTcalc\")\n",
    "flow.integrate.clean_data(\"test_OW1\", remove_nulls=True)\n",
    "flow.integrate.clean_data(\"test_OW1\", char_col=\"WHT\", remove_char=\"-\")\n",
    "flow.integrate.edit_col(\"test_OW1\", \"pbh\", newname=\"DHP\")\n",
    "flow.integrate.edit_col(\"test_OW1\", \"Qgl\", newname=\"GLR\")\n",
    "flow.integrate.edit_col(\"test_OW1\", \"Q liq\", newname=\"Qliq\")\n",
    "flow.integrate.edit_col(\"test_OW1\", \"WHT\", typ=\"double\")\n",
    "flow.integrate.organise_data(\"test_OW1\", \"mult_col\")\n",
    "flow.integrate.set_organised(\"test_OW1\")\n",
    "flow.integrate.cache_data(\"test_OW1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make datasets for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.next_phase()\n",
    "# Make Spark DataFrame sets: Pick the features we want in our datasets from the different sources of data!\n",
    "flow.datasets.make_set(\"Sep\", align_dates=\"day\", feats={\"OW1\": [\"GLP\", \"ASD\"], \"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"Field\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"ASD\", \"WHP\", \"GLP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})\n",
    "\n",
    "flow.datasets.make_set(\"Sep_2018\", align_dates=\"day\", feats={\"OW1\": [\"GLP\", \"ASD\"], \"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"Field_2018\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"ASD\", \"WHP\", \"GLP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})\n",
    "\n",
    "flow.datasets.make_set(\"OrigSep\", align_dates=\"day\", feats={\"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"OrigField\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"WHP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})\n",
    "\n",
    "flow.datasets.make_set(\"OrigSep_2018\", align_dates=\"day\", feats={\"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"OrigField_2018\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"WHP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate the datasets: We want to move some rows of data into the our other Spark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.datasets.cache_data(\"Sep\", \"Field\")\n",
    "condition = lambda df : df.where(df.Qliq <= 50)\n",
    "flow.datasets.append_rows(\"Field\", condition, \"Sep\")\n",
    "flow.datasets.date_range(\"OrigSep_2018\", \"2018-06-01\", \"2020-01-01\")\n",
    "flow.datasets.date_range(\"OrigField_2018\", \"2018-06-01\", \"2020-01-01\")\n",
    "\n",
    "condition = lambda df : df.where(df.Qliq <= 50)\n",
    "flow.datasets.append_rows(\"Field_2018\", condition, \"Sep_2018\")\n",
    "flow.datasets.date_range(\"Sep_2018\", \"2018-06-01\", \"2020-01-01\")\n",
    "flow.datasets.date_range(\"Field_2018\", \"2018-06-01\", \"2020-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make our dataset objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark dataframes to pandas dataset\n",
    "flow.datasets.make_dataset(\"Sep\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"Field\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"Sep_2018\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"Field_2018\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigSep\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigField\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigSep_2018\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigField_2018\", label=\"Qliq\", pandas=True)\n",
    "\n",
    "flow.next_phase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply a range of ML models to one of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = flow.dataexplore.get_datasets()\n",
    "eval_preds = flow.dataexplore.fitpredict_naivemodels(datasets[\"OrigSep_2018\"], datasets[\"OrigField_2018\"], eval_all=True)\n",
    "flow.dataexplore.plotmodels(datasets[\"OrigSep_2018\"], datasets[\"OrigField_2018\"], eval_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
