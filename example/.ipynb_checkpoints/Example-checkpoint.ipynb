{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark should be installed as a dependency\n",
    "import mlflowrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-e2916bc76823>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-e2916bc76823>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    mlflowrate.\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mlflowrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datas into notebook\n",
    "df_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\n",
    "df_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\n",
    "df_OW1ql = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\n",
    "df_records = spark.read.format('csv').options(header='true', inferSchema='true', delimiter='|', encoding='iso-8859-1').load('/FileStore/tables/interences_filtered.csv')\n",
    "test_OW1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/OW1test_1.csv')\n",
    "\n",
    "# Convert date columns to datetime formats\n",
    "# Rename columns so that it fits with flow package.\n",
    "df_01 = df_01.select(\n",
    "      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n",
    "      df_01[\"name\"].alias(\"tag\"),\n",
    "      df_01[\"value\"])\n",
    "\n",
    "df_02 = df_02.select(\n",
    "      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n",
    "      df_02[\"name\"].alias(\"tag\"),\n",
    "      df_02[\"value\"])\n",
    "\n",
    "df_OW1ql = df_OW1ql.select(\n",
    "      F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "      *[feat for feat in df_OW1ql.columns if feat not in [\"DATE\"]])\n",
    "\n",
    "test_OW1 = test_OW1.select(\n",
    "      F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy HH:mm\").alias(\"datetime\"),\n",
    "      *[n for n in test_OW1.schema.names if not n == \"DATE\"])\n",
    "\n",
    "df_records = df_records.select(\n",
    "      F.to_timestamp(F.col(\"Date\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "      *[n for n in df_records.schema.names if not n == \"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create dictionaries for translating data tagnames\n",
    "do = {\"OW1\" : [\"EXAMPLE\", \"EXAMPLE\"],\n",
    "      \"OW2\" : [\"EXAMPLE\", \"EXAMPLE\"],\n",
    "      \"OW3\" : [\"EXAMPLE\"]}\n",
    "\n",
    "nn = {\"EXAMPLE\" : \"WHP\",\n",
    "      \"EXAMPLE\" : \"WHT\",\n",
    "      \"EXAMPLE\" : \"GLR\",\n",
    "      \"EXAMPLE\" : \"GLP\",\n",
    "      \"EXAMPLE\" : \"DHP\",\n",
    "      \"EXAMPLE\" : \"DHT\",\n",
    "      \"EXAMPLE\" : \"Choke\",\n",
    "      \"EXAMPLE\" : \"ASD\"}\n",
    "\n",
    "# Collect up data into a dictionary with corresponding names\n",
    "data = {\"df_01\":df_01, \"df_02\":df_02, \"df_OW1ql\":df_OW1ql, \"test_OW1\":test_OW1, \"df_records\":df_records}\n",
    "\n",
    "# Make new workflow object by passing in the data dictionary \n",
    "flow = WorkFlow(data)\n",
    "\n",
    "# Cache data to speed up spark queries\n",
    "flow.data.cache_data(\"df_01\", \"df_02\", \"df_OW1ql\", \"test_OW1\", \"df_records\")\n",
    "\n",
    "# Merge dataframes vertically\n",
    "flow.data.merge_data(\"df_dump\", \"df_01\", \"df_02\", axis=0)\n",
    "\n",
    "# select the stored data, and remove nulls\n",
    "flow.data.clean_data(\"df_dump\", remove_nulls=True)\n",
    "\n",
    "# select the stored data, and reorganise it to more dictionaries\n",
    "flow.data.organise_data(\"df_dump\", \"date_tag_val_col\", distinct_oilwells=do, change_sensor_names=nn) \n",
    "\n",
    "# We get three new datasets OW1. OW2. OW3. but their dataframes will be out of shape\n",
    "flow.data.clean_data(\"OW1\", avg_over=\"day\", is_dict=True)  # average the data on the dict format\n",
    "flow.data.organise_data(\"OW1\", \"dict_col\")  # reorganise the data again so the dictionary matches the dataframe\n",
    "flow.data.set_organised(\"OW1\") # Tell the class that this data is ready for dataset assembly\n",
    "flow.data.cache_data(\"OW1\") # Cache data frame for fast querying\n",
    "flow.data.status(\"OW1\") # check the status of the data\n",
    "\n",
    "flow.data.clean_data(\"df_OW1ql\", remove_nulls=True) # remove nulls in the data\n",
    "flow.data.edit_col(\"df_OW1ql\", \"Daily liquid rate [Sm3/d]\", newname=\"Qliq\") # edit the column files to have a new name\n",
    "flow.data.select_col(\"df_OW1ql\", \"Qliq\") # reselect the data frame to only be the given sample\n",
    "flow.data.organise_data(\"df_OW1ql\", \"mult_col\") # reorganise data so that it we have a correponding group dict format\n",
    "flow.data.set_organised(\"df_OW1ql\") # Tell the class that this data is ready for assembly\n",
    "flow.data.cache_data(\"df_OW1ql\") # cache data frame for fast querying\n",
    "flow.data.status(\"df_OW1ql\") # check the status of the data\n",
    "\n",
    "flow.data.drop_col(\"test_OW1\", \"WELLNAME\", \"NUMBER\", \"Qo\", \"Qg\", \"Qw\", \"GOR\", \"WOR\", \"WTC\", \"PI\", \"Pres\", \"DCP\", \"ChokeD\", \"DCTcalc\")\n",
    "flow.data.clean_data(\"test_OW1\", remove_nulls=True)\n",
    "flow.data.clean_data(\"test_OW1\", char_col=\"WHT\", remove_char=\"-\")\n",
    "flow.data.edit_col(\"test_OW1\", \"pbh\", newname=\"DHP\")\n",
    "flow.data.edit_col(\"test_OW1\", \"Qgl\", newname=\"GLR\")\n",
    "flow.data.edit_col(\"test_OW1\", \"Q liq\", newname=\"Qliq\")\n",
    "flow.data.edit_col(\"test_OW1\", \"WHT\", typ=\"double\")\n",
    "flow.data.organise_data(\"test_OW1\", \"mult_col\")\n",
    "flow.data.set_organised(\"test_OW1\")\n",
    "flow.data.cache_data(\"test_OW1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.next_phase()\n",
    "# make sets first, these are basically dataframes.\n",
    "flow.datasets.make_set(\"Sep\", align_dates=\"day\", feats={\"OW1\": [\"GLP\", \"ASD\"], \"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"Field\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"ASD\", \"WHP\", \"GLP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})\n",
    "\n",
    "flow.datasets.make_set(\"Sep_2018\", align_dates=\"day\", feats={\"OW1\": [\"GLP\", \"ASD\"], \"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"Field_2018\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"ASD\", \"WHP\", \"GLP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})\n",
    "\n",
    "flow.datasets.make_set(\"OrigSep\", align_dates=\"day\", feats={\"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"OrigField\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"WHP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})\n",
    "\n",
    "flow.datasets.make_set(\"OrigSep_2018\", align_dates=\"day\", feats={\"test_OW1\": [\"WHP\", \"Choke\", \"GLR\", \"DHP\", \"Qliq\", \"WHT\"]})\n",
    "flow.datasets.make_set(\"OrigField_2018\", align_dates=\"day\", feats={\"OW1\": [\"DHP\", \"WHT\", \"GLR\", \"WHP\", \"Choke\"], \"df_OW1ql\":[\"Qliq\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.datasets.cache_data(\"Sep\", \"Field\")\n",
    "condition = lambda df : df.where(df.Qliq <= 50)\n",
    "flow.datasets.append_rows(\"Field\", condition, \"Sep\")\n",
    "flow.datasets.date_range(\"OrigSep_2018\", \"2018-06-01\", \"2020-01-01\")\n",
    "flow.datasets.date_range(\"OrigField_2018\", \"2018-06-01\", \"2020-01-01\")\n",
    "\n",
    "condition = lambda df : df.where(df.Qliq <= 50)\n",
    "flow.datasets.append_rows(\"Field_2018\", condition, \"Sep_2018\")\n",
    "flow.datasets.date_range(\"Sep_2018\", \"2018-06-01\", \"2020-01-01\")\n",
    "flow.datasets.date_range(\"Field_2018\", \"2018-06-01\", \"2020-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark dataframes to pandas dataset\n",
    "flow.datasets.make_dataset(\"Sep\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"Field\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"Sep_2018\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"Field_2018\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigSep\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigField\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigSep_2018\", label=\"Qliq\", pandas=True)\n",
    "flow.datasets.make_dataset(\"OrigField_2018\", label=\"Qliq\", pandas=True)\n",
    "\n",
    "flow.next_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "datasets = flow.dataexplore.get_datasets()\n",
    "eval_preds = flow.dataexplore.fitpredict_naivemodels(datasets[\"OrigSep_2018\"], datasets[\"OrigField_2018\"], eval_all=True)\n",
    "flow.dataexplore.plotmodels(datasets[\"OrigSep_2018\"], datasets[\"OrigField_2018\"], eval_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
