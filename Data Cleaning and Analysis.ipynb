{"cells":[{"cell_type":"code","source":["# Import Modules\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, TimestampType\nfrom pyspark.sql.functions import coalesce, lit, col, lead, lag\nfrom pyspark.sql.functions import stddev, mean, col\nfrom pyspark.sql.window import Window\n\nfrom operator import add\nfrom functools import reduce\n\n# Standard Python Modules\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport pandas as pd\nimport numpy as np\nimport re"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["class DataframeTools:\n  \"\"\"Parent class for manipulating spark dataframes. \"\"\"\n  \n  def __init__(self, df):\n    self.df = df\n  \n  \n  def update_df(self, new_df):\n    \"\"\"Replace class dataframe with a new one.\n    \n    Args:\n      new_df (dataframe): New dataframe\n      \n    \"\"\"\n    \n    self.df = new_df\n  \n  \n  def append_data(self, new_df):\n    \"\"\"Append another dataframe below the current one. \n    New dataframe must have the same columns as the original dataframe.\n    \n    Args:\n      new_df (dataframe): New dataframe\n      \n    \"\"\"\n    \n    assert self.df.schema == new_df.schema, \"Column headers must match!\"\n    \n    print(\"Current samples: \", self.df.count())\n    print(\"Appending samples: \", new_df.count())\n    \n    self.df = self.df.union(new_df)\n    print(\"Joined samples: \", self.df.count())\n    print(\"\")\n  \n  \n  def is_null(self, dfs):\n    \"\"\"Check all columns in dataframe for null values.\n    \n    Args:\n      dfs (dataframe): Dataframe to be checked on\n      \n    \"\"\"\n    \n    print(\"Number of samples with a null value across columns:\")\n    for col in dfs.schema:\n      head = col.name\n      #print(dfs[head].isNull() == True)\n      print(head, dfs.where(dfs[head].isNull() == True).count())\n    print(\"\")\n    \n    \n  def null2zero(self, head, dfs):\n    \"\"\"Change null values to zero in column\n    \n    Args:\n      head (str): Column name\n      dfs (dataframe): Dataframe given\n      \n    \"\"\"\n    \n    print(\"Replacing null values with zero...\\n\")\n    dfs = dfs.na.fill(0, (head))\n    return dfs\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["class GroupDataTools(DataframeTools):\n  \"\"\"Subclass of Dataframe tools to reorganise dataframes into dictionaries.\n    Tools for visualisation and preprocessing included.\n  \"\"\"\n  \n  def __init__(self, df, df_dict={}):\n    super().__init__(df)\n    self.df_dict = df_dict\n    self.headers = [h.name for h in df.schema]\n    \n    \n  def groupdata(self, dict_group_head, x_head, y_head):\n    \"\"\"Collect rows which contain the same value in dict_group_head column and \n    put them into a dictionary. \n    \n    Keys: distinct value, Value: dataframe whose rows contain distinct value\n    \n    Args:\n      dict_group_head (str): column to split the dataframe\n      x_head (str): first column (normally time) to reconstruct corresponding dataframe\n      y_head (str): second column (normally value) to reconstruct corresponding dataframe\n    \n    \"\"\"\n    \n    # Split dataframe up by given header\n    assert dict_group_head in self.headers, \"Header does not exist in dataframe!\"\n    \n    unq_items = self.df.select(dict_group_head).distinct()\n    n = unq_items.rdd.map(lambda x: x[dict_group_head]).collect()\n\n    for key in n:\n      self.df_dict[key] = self.df.orderBy(x_head).where(self.df[dict_group_head] == key).select(self.df[x_head], self.df[y_head])\n  \n  \n  def splitdata_dict(self, regex):\n    \"\"\"Make a dictionary from df_dict given a conditional substring which matches within the keys of df_dict.\n    E.g. Return a dictionary of a certain oilwell given from a code in the tag name.\n\n    Args:\n      regex (str): substring to match with keys, written in wildcard format for regular expressions\n      \n    \"\"\"\n    \n    out_dict = {}\n    \n    for (key, val) in self.df_dict.items():\n      for reg in regex:\n        if re.match(reg, key):\n          out_dict[key] = val\n      \n    return out_dict\n    \n    \n  def decode_keys(self, in_dict, decode_dict):\n    \"\"\"Replace the old keys with new key definitions.\n\n      Args:\n        in_dict (dict): input dictionary \n        decode_dict (dict): dictionary of old keys with new definitions\n\n      Returns: None\n      \n    \"\"\"\n\n    for key, val in in_dict.items():\n      for k, new_key in decode_dict.items():\n        regex = re.compile(k)\n        if (re.match(regex, key)):\n          print(\"Replacing \", key, \", with \", new_key)\n          in_dict[new_key] = in_dict.pop(key)\n    print(\"\")\n    \n    return None\n  \n  \n  def plot_ts(self, title, x_head, y_head, ts_df_list, label_list=[\"value\"]):\n    \"\"\"Plot multiple timeseries dataframe onto a figure, x axis = time, y axis = value.\n\n    Args:\n      title (str): Name of dataframe\n      x_head (str): Name of column to be plotted along x axis\n      y_head (str): Name of column to be plotted along y axis\n      ts_df_list (list): list of timeseries dataframes to plot\n      label_list (list): list of plot labels\n\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(24, 10))\n\n    for ts_df, lab in zip(ts_df_list, label_list):\n      ts_pd = ts_df.orderBy(x_head).toPandas()\n\n      y = ts_pd[y_head].tolist()\n      x = ts_pd[x_head].tolist()\n\n      ax.plot(x, y, \".--\", label=lab)\n\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel(x_head, fontsize=16)\n    ax.set_ylabel(y_head, fontsize=16)\n\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n    ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n\n    ax.legend(loc=\"best\")\n\n    fig.tight_layout()\n    display(fig)\n    \n\n  def weighted_average(ts_df, x_head, y_head, offsets, weights):\n    \"\"\"Produce rolling average results of the given ts data with the given specs.\n\n      Args:\n        ts_df (dataframe): timeseries dataframe\n        x_head (str): header name of x axis in timeseries (e.g. datetime)\n        y_head (str): header name of y axis in timeseries (e.g. value)\n        offsets (list): list of adjacent values to consider\n        weights (list): list of weights applied to offsets\n\n    \"\"\"\n    window = Window.orderBy(x_head)\n    v = col(y_head)\n\n    assert len(weights) == len(offsets)\n\n    def value(i):\n        if i < 0: return lag(v, -i).over(window)\n        if i > 0: return lead(v, i).over(window)\n        return v\n\n    values = [coalesce(value(i) * w, lit(0))/len(offsets) for i, w in zip(offsets, weights)]\n\n    return reduce(add, values, lit(0))\n  \n  \n  def view_moving_avg(title, x_head, y_head, y_label, ts_df, offsets, weights):\n    \"\"\"Wrapper function to view the moving average of the given ts data\n\n      Args:\n        title (str): Title of graph\n        x_head (str): Header name for x column data\n        y_head (str): Header name for y column data\n        y_label (str): y axis label\n        ts_df (dataframe): Time series dataframe\n        offsets (list): list of adjacent values to consider\n        weights (list): list of weights applied to offsets\n\n    \"\"\"\n\n    avg = ts_df.withColumn(\"avg\", weighted_average(ts_df, offsets, weights)).drop(y_head)\n    avg = avg.select(avg[x_head],\n                     avg[\"avg\"].alias(y_head))\n\n    plot_ts(title, y_label, [avg])\n    \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Load data into the notebook\ndf_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\n\n# Rename and cast types for each column\ndf_01 = df_01.select(\n      df_01[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_01[\"name\"].alias(\"tag\"),\n      df_01[\"value\"]\n)\n\ndf_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\n\ndf_02 = df_02.select(\n      df_02[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_02[\"name\"].alias(\"tag\"),\n      df_02[\"value\"]\n)\n\ndisplay(df_02.select(\"tag\").distinct())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tag</th></tr></thead><tbody><tr><td>BRA-PT  -16-010</td></tr><tr><td>BRA-TT  -15-074</td></tr><tr><td>BRA-QT  -15-0017-RAW</td></tr><tr><td>BRA-FIC -16-071</td></tr><tr><td>BRA-QT  -15-0077-RAW</td></tr><tr><td>BRA-PT  -16-070</td></tr><tr><td>BRA-HV  -15-012</td></tr><tr><td>BRA-HV  -15-072</td></tr><tr><td>BRA-FIC -16-041</td></tr><tr><td>BRA-PT  -16-040</td></tr></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"code","source":["explore = GroupDataTools(df_01)\nexplore.append_data(df_02)\nexplore.is_null(explore.df)\nexplore.df = explore.null2zero(\"value\", explore.df)\nexplore.is_null(explore.df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Current samples:  865484\nAppending samples:  601804\nJoined samples:  1467288\n\nNumber of samples with a null value across columns:\nindex 0\ndatetime 0\ntag 0\nvalue 116433\n\nReplacing null values with zero...\n\nNumber of samples with a null value across columns:\nindex 0\ndatetime 0\ntag 0\nvalue 0\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["explore.groupdata(\"tag\", \"datetime\", \"value\")\n\nr1 = re.compile('BRA-....-..-07.')\nr2 = re.compile('BRA-QT  -15-0077-RAW')\nA07 = explore.splitdata_dict([r1, r2])\n\nr1 = re.compile('BRA-....-..-01.')\nr2 = re.compile('BRA-QT  -15-0017-RAW')\nA01 = explore.splitdata_dict([r1, r2])\n\nr1 = re.compile('BRA-....-..-04.')\nA04 = explore.splitdata_dict([r1])\n\n# Make a tag dictionary: Decode the tags!\ntag_names = {\n              \"BRA-PZT........\" : \"WHP\",\n              \"BRA-TT  -15....\" : \"WHT\",\n              \"BRA-FI.........\" : \"GLR\",\n              \"BRA-PT  -16....\" : \"GLP\",\n              \"BRA-PT  -13....\" : \"DHP\",\n              \"BRA-TT  -13....\" : \"DHT\",\n              \"BRA-HV.........\" : \"Choke\",\n              \"BRA-QT.........\" : \"ASD\"\n}\n\nexplore.decode_keys(A07, tag_names)\nexplore.decode_keys(A01, tag_names)\nexplore.decode_keys(A04, tag_names)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Replacing  BRA-HV  -15-072 , with  Choke\nReplacing  BRA-TT  -15-074 , with  WHT\nReplacing  BRA-PZT -13-074 , with  WHP\nReplacing  BRA-TT  -13-076 , with  DHT\nReplacing  BRA-FIC -16-071 , with  GLR\nReplacing  BRA-PT  -13-077 , with  DHP\nReplacing  BRA-QT  -15-0077-RAW , with  ASD\nReplacing  BRA-PT  -16-070 , with  GLP\n\nReplacing  BRA-PZT -13-014 , with  WHP\nReplacing  BRA-FI  -16-011 , with  GLR\nReplacing  BRA-TT  -15-014 , with  WHT\nReplacing  BRA-QT  -15-0017-RAW , with  ASD\nReplacing  BRA-PT  -16-010 , with  GLP\nReplacing  BRA-HV  -15-012 , with  Choke\n\nReplacing  BRA-HV  -15-042 , with  Choke\nReplacing  BRA-FIC -16-041 , with  GLR\nReplacing  BRA-TT  -15-044 , with  WHT\nReplacing  BRA-PT  -16-040 , with  GLP\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["ts_dfs = [A07[\"DHP\"], A07[\"WHP\"], A07[\"DHT\"], A07[\"WHT\"], A07[\"GLP\"]]\nts_labels = [\"DHP\", \"WHP\", \"DHT\", \"WHT\", \"GLP\"]\n\nge2016 = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHP, WHT, DHP, DHT, GLP over time\", \"datetime\", \"value\", ge2016, ts_labels)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/ba398607-b8c1-43a4-ab35-4dd0b5d02331.png"]}}],"execution_count":7},{"cell_type":"code","source":["ts_dfs = [A07[\"DHP\"], A07[\"WHP\"], A07[\"GLP\"]]\nts_labels = [\"DHP\", \"WHP\", \"GLP\"]\n\nge2016p = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHP, DHP, GLP over time\", \"datetime\", \"value\", ge2016p, ts_labels)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/9677425e-4161-4cdc-8385-80b92ff37206.png"]}}],"execution_count":8},{"cell_type":"code","source":["ts_dfs = [A07[\"DHT\"], A07[\"WHT\"]]\nts_labels = [\"DHT\", \"WHT\"]\n\nge2016t = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHT, DHT over time\", \"datetime\", \"value\", ge2016t, ts_labels)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/2f0c1a69-77a6-470b-887e-7159c34192cd.png"]}}],"execution_count":9},{"cell_type":"code","source":["# Load new data containing rates into the notebook\nA07_ql_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\n\n# Add the relevant columns to the oilwell dictionary\nA07[\"LR\"] = A07_ql_df.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    A07_ql_df[\"Daily liquid rate [Sm3/d]\"].alias(\"value\")\n)\n\nA07[\"OR\"] = A07_ql_df.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    A07_ql_df[\"Daily oil [Sm3/d]\"].alias(\"value\")\n)\n\n# - Compare if WHP, DHP, GLR are the same as the dump ones!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [A07[\"DHP\"], A07[\"WHP\"], A07[\"GLP\"], A07[\"OR\"], A07[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"GLP\", \"OR\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Normalised WHP, DHP, GLP, OR, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/84852d16-7623-46ca-bf96-e9192e403cb8.png"]}}],"execution_count":11},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [A07[\"DHP\"], A07[\"WHP\"], A07[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Normalised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/1ef61150-7f07-4686-b16b-2cb153a704cb.png"]}}],"execution_count":12},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [A07[\"DHP\"], A07[\"WHP\"], A07[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\")) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Standardised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/9d44c194-bd67-4229-91c3-a58526e3ae40.png"]}}],"execution_count":13},{"cell_type":"code","source":["# offsets, weights =  [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n# view_moving_avg(\"Moving average of A07 WHP\", \"WHP (Bar)\", A07[\"GLR\"], offsets, weights)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/cd88a05b-9769-41c8-92a6-4c51d88168d2.png"]}}],"execution_count":14},{"cell_type":"code","source":["# Fill in missing data\n# Remove anomalies...\n\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"Data Cleaning and Analysis","notebookId":3265004742220724},"nbformat":4,"nbformat_minor":0}