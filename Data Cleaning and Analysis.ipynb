{"cells":[{"cell_type":"code","source":["# Import Modules\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, TimestampType, DoubleType\nfrom pyspark.sql.functions import coalesce, lit, col, lead, lag\nfrom pyspark.sql.functions import stddev, mean, col\nfrom pyspark.sql.window import Window\n\nfrom operator import add\nfrom functools import reduce\n\nfrom googletrans import Translator\n\n# Standard Python Modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mdates\nimport pandas as pd\nimport numpy as np\nimport re"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["class DataframeTools:\n  \"\"\"Parent class for manipulating spark dataframes. \"\"\"\n  \n  def __init__(self, df):\n    self.df = df\n    \n  \n  def update_df(self, new_df):\n    \"\"\"Replace class dataframe with a new one.\n    \n    Args:\n      new_df (dataframe): New dataframe\n      \n    \"\"\"\n    \n    self.df = new_df\n  \n  \n  def append_data(self, new_df):\n    \"\"\"Append another dataframe below the current one. \n    New dataframe must have the same columns as the original dataframe.\n    \n    Args:\n      new_df (dataframe): New dataframe\n      \n    \"\"\"\n    \n    assert self.df.schema == new_df.schema, \"Column headers must match!\"\n    \n    print(\"Current samples: \", self.df.count())\n    print(\"Appending samples: \", new_df.count())\n    \n    self.df = self.df.union(new_df)\n    print(\"Joined samples: \", self.df.count())\n    print(\"\")\n  \n  \n  def is_null(self, dfs):\n    \"\"\"Check all columns in dataframe for null values.\n    \n    Args:\n      dfs (dataframe): Dataframe to be checked on\n      \n    \"\"\"\n    \n    print(\"Number of samples with a null value across columns:\")\n    for col in dfs.schema:\n      head = col.name\n      #print(dfs[head].isNull() == True)\n      print(head, dfs.where(dfs[head].isNull() == True).count())\n    print(\"\")\n    \n    \n  def null2zero(self, head, dfs):\n    \"\"\"Change null values to zero in column\n    \n    Args:\n      head (str): Column name\n      dfs (dataframe): Dataframe given\n    \n    Returns:\n      dataframe with replaced null values\n      \n    \"\"\"\n    \n    print(\"Replacing null values with zero...\\n\")\n    dfs = dfs.na.fill(0, (head))\n    return dfs\n\n  \n  def translate_col(self, head, dfs, src='no', dest='en'):\n    \"\"\"Translate a string column of a dataframe from src language to dest language\n    \n    Args:\n      head (str): Column name\n      dfs (dataframe): Dataframe given\n      src (str): Source language to translate from\n      dest (str): Target language to translate to\n    \n    Returns:\n      new_dfs (dataframe): dataframe with translated column\n      translation_dict (dict): dictionary of distinct strings translated\n      \n    \"\"\"\n    \n    # Translate descriptions\n    translator = Translator()\n    # Select distinct comments\n    n = dfs.select(dfs[head]).distinct().rdd.map(lambda x: x[head]).collect()\n    # Make a dictionary to translate distinct comments\n    translation_dict = {col : translator.translate(col, src=src, dest=dest).text for col in n}\n    # Utilise spark method and replace all norweigan comments with translated ones\n    new_dfs = dfs.na.replace(translation_dict, 1, head)\n    return new_dfs, translation_dict\n  \n  \n  def ts_overlay_records(self, df_ts, df_records, head, filt_dict=None, translate_dict=None):\n    \"\"\"Given a ts dataframe, make a new column and match corresponding records according to time.\n       Pass in filt_dict and translate_dict to be able to group up similar records.\n       \n    Args:\n      df_ts (dataframe): Time series dataframe\n      df_records (dataframe): Time series dataframe whose values contain records (e.g. string descriptions)\n      head (str): Column name of the records in df_records\n      filt_dict (dict): Dictionary of similar substrings found in similar records and it's new collective record\n      translate_dict (dict): Dictionary of translated distinct records\n      \n    Returns:\n      new_df (dataframe): dataframe containing new column of time correlated records to the time series and other useful columns\n    \n    \"\"\"\n\n    # Make a new column where datetime precision: daily. (because interferences are recorded daily)\n    df = df_ts.select(\n      df_ts[\"datetime\"].alias(\"datetime_orig\"),\n      (F.round(F.unix_timestamp(F.col(\"datetime\")) / 86400) * 86400).cast(\"timestamp\").alias(\"datetime\"),\n      df_ts[\"value\"]\n    )\n\n    new_df = df.join(df_records, on=['datetime'], how='left_outer')\n    \n    if (filt_dict is not None) and (translate_dict is not None):\n      print(\"\\nCollecting similar comments given the filt_dict...\")\n      group_dict = {}\n\n      for comment in translate_dict.values():\n        for abrv, group_comment in filt_dict.items():\n          if abrv in comment:\n            group_dict[comment] = group_comment\n            #print(comment, \" : \", group_comment)\n\n      new_df = new_df.withColumn(\"Grouped\", new_df[head])\n      new_df = new_df.na.replace(group_dict, 1, \"Grouped\")\n      new_df = new_df.na.fill(\"No Records\", \"Grouped\")\n    \n    new_df = new_df.na.fill(\"No Records\", head)\n    \n    new_df = new_df.drop(\"datetime\")\n    new_df = new_df.withColumnRenamed(\"datetime_orig\", \"datetime\")\n    \n    return new_df\n    \n    \n  def discretise_col(self, dfs, head):\n    \"\"\"Make new column for a dataframe which numerically discretises a descriptive column.\n\n    Args: \n      dfs (dataframe): Dataframe which contains: datetime, value, Description, Grouped\n      head (str): Name of the column to discretise\n\n    Returns:\n      dataframe with column of discretised values of the descriptive column\n\n    \"\"\"\n\n    # Make column by numerically discretising distinct comments\n    grouped_desc_list = dfs.select(dfs[head]).distinct().rdd.map(lambda x: x[head]).collect()\n\n    # Convert comments to discrete values\n    numerate = {str(val) : str(i) for i, val in enumerate(grouped_desc_list)}\n\n    new_dfs = dfs.withColumn(\"Discrete_str\", dfs[head])\n\n    new_dfs = new_dfs.na.replace(numerate, 1, \"Discrete_str\")\n    new_dfs = new_dfs.withColumn(\"Discrete\", new_dfs[\"Discrete_str\"].cast(IntegerType())).drop(\"Discrete_str\")\n    new_dfs = new_dfs.na.fill(0, (\"Discrete\"))\n\n    return new_dfs\n      \n      \n  def add_year_col(self, dfs):\n    \"\"\"Add column of year of date to dataframe.\n\n    Args:\n      dfs (dataframe): dataframe with datetime\n\n    Returns:\n      dataframe with column for year.\n\n    \"\"\"\n\n    return dfs.withColumn(\"year\", F.year(F.col(\"datetime\")))\n  \n  \n  def merge_duplicate(self, dfs):\n    \"\"\"Collect up duplicated datetimes with different descriptions, and merge the descriptions together.\n\n      Args:\n        dfs (dataframe): dataframe in the format: datetime|value|description|groupedDescription\n\n      Returns:\n        dataframe with merged descriptions\n\n    \"\"\"\n\n    reduced = dfs\\\n        .rdd\\\n        .map(lambda row: (row[0], [(row[1], row[2], row[3])]))\\\n        .reduceByKey(lambda x,y: x+y)\\\n        .map(lambda row: (\n                row[0],                                #key i.e. datetime\n                row[1][0][0],    #sum(row[1][0]) / len(row[1][0]),       #value, take the average of the values \n                ','.join([str(e[1]) for e in row[1]]), #join up the descriptions\n                ','.join([str(e[2]) for e in row[1]])  #join up the grouped descriptions\n            )\n        )\n\n    schema_red = dfs.schema\n    new_dfs = sqlContext.createDataFrame(reduced, schema_red).orderBy(\"datetime\")\n\n    if new_dfs.count() > new_dfs.dropDuplicates([\"datetime\"]).count():\n      raise ValueError('Data has duplicates')\n\n    return new_dfs\n\n  \n  def avg_over_period(self, dfs, period=\"day\"):\n    \"\"\"Given a dataframe with datetime column, average over days, weeks, months or years and return new dataframe. \n      \n      Args:\n        dfs (dataframe): dataframe in the format of: datetime|value|...|...\n      \n      Returns:\n        dataframe in format of: datetime|value  , where value is the averaged value over the period\n    \n    \"\"\"\n    \n    dfs_new = dfs.withColumn(period, F.date_trunc(period, dfs.datetime))\n    \n    dfs_new = dfs_new\\\n                  .groupBy(period)\\\n                  .agg(F.avg(\"value\"))\\\n                  .orderBy(\"day\")\n    \n    dfs_new = dfs_new.withColumnRenamed(\"avg(value)\", \"value\")\n    \n    return dfs_new\n  \n  \n  def threshold(self, dfs):\n    mean, std = dfs.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n    \n    dfs.where(dfs.value >= )\n  \n  \n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["class GroupDataTools(DataframeTools):\n  \"\"\"Subclass of Dataframe tools to reorganise dataframes into dictionaries.\n    Tools for visualisation and preprocessing included.\n  \"\"\"\n  \n  def __init__(self, df, df_dict={}):\n    super().__init__(df)\n    self.df_dict = df_dict\n    self.headers = [h.name for h in df.schema]\n    \n    \n  def groupdata(self, dict_group_head, x_head, y_head):\n    \"\"\"Collect rows which contain the same value in dict_group_head column and \n    put them into a dictionary. \n    \n    Keys: distinct value, Value: dataframe whose rows contain distinct value\n    \n    Args:\n      dict_group_head (str): column to split the dataframe\n      x_head (str): first column (normally time) to reconstruct corresponding dataframe\n      y_head (str): second column (normally value) to reconstruct corresponding dataframe\n    \n    \"\"\"\n    \n    # Split dataframe up by given header\n    assert dict_group_head in self.headers, \"Header does not exist in dataframe!\"\n    \n    unq_items = self.df.select(dict_group_head).distinct()\n    n = unq_items.rdd.map(lambda x: x[dict_group_head]).collect()\n\n    for key in n:\n      self.df_dict[key] = self.df.orderBy(x_head).where(self.df[dict_group_head] == key).select(self.df[x_head], self.df[y_head])\n  \n  \n  def splitdata_dict(self, regex):\n    \"\"\"Make a dictionary from df_dict given a conditional substring which matches within the keys of df_dict.\n    E.g. Return a dictionary of a certain oilwell given from a code in the tag name.\n\n    Args:\n      regex (str): substring to match with keys, written in wildcard format for regular expressions\n      \n    \"\"\"\n    \n    out_dict = {}\n    \n    for (key, val) in self.df_dict.items():\n      for reg in regex:\n        if re.match(reg, key):\n          out_dict[key] = val\n      \n    return out_dict\n    \n    \n  def decode_keys(self, in_dict, decode_dict):\n    \"\"\"Replace the old keys with new key definitions.\n\n      Args:\n        in_dict (dict): input oilwell dictionary containing dataframes\n        decode_dict (dict): dictionary of old keys with new definitions\n\n      Returns: None\n      \n    \"\"\"\n    \n    new_dict = {}\n    \n    for key, val in in_dict.items():\n      for k, new_key in decode_dict.items():\n        regex = re.compile(k)\n        if (re.match(regex, key)):\n          #print(\"Replacing \", key, \", with \", new_key)\n          new_dict[new_key] = val\n          #for i, j in new_dict.items():\n            #print(i, \" : \", j)\n    print(\"\")\n    \n    return new_dict\n  \n  \n  def plot_ts(self, title, x_head, y_head, ts_df_list, label_list=[\"value\"]):\n    \"\"\"Plot multiple timeseries dataframe onto a figure, x axis = time, y axis = value.\n\n    Args:\n      title (str): Name of dataframe\n      x_head (str): Name of column to be plotted along x axis\n      y_head (str): Name of column to be plotted along y axis\n      ts_df_list (list): list of timeseries dataframes to plot\n      label_list (list): list of plot labels\n\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(24, 10))\n\n    for ts_df, lab in zip(ts_df_list, label_list):\n      ts_pd = ts_df.orderBy(x_head).toPandas()\n\n      y = ts_pd[y_head].tolist()\n      x = ts_pd[x_head].tolist()\n\n      ax.plot(x, y, \".--\", label=lab)\n\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel(x_head, fontsize=16)\n    ax.set_ylabel(y_head, fontsize=16)\n\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n    ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n\n    ax.legend(loc=\"best\")\n\n    fig.tight_layout()\n    display(fig)\n    \n\n  def weighted_average(self, ts_df, x_head, y_head, offsets, weights):\n    \"\"\"Produce rolling average results of the given ts data with the given specs.\n\n      Args:\n        ts_df (dataframe): timeseries dataframe\n        x_head (str): header name of x axis in timeseries (e.g. datetime)\n        y_head (str): header name of y axis in timeseries (e.g. value)\n        offsets (list): list of adjacent values to consider\n        weights (list): list of weights applied to offsets\n\n    \"\"\"\n    window = Window.orderBy(x_head)\n    v = col(y_head)\n\n    assert len(weights) == len(offsets)\n\n    def value(i):\n        if i < 0: return lag(v, -i).over(window)\n        if i > 0: return lead(v, i).over(window)\n        return v\n\n    values = [coalesce(value(i) * w, lit(0))/len(offsets) for i, w in zip(offsets, weights)]\n\n    return reduce(add, values, lit(0))\n  \n  \n  def view_moving_avg(self, title, x_head, y_head, y_label, ts_df, offsets, weights):\n    \"\"\"Wrapper function to view the moving average of the given ts data\n\n      Args:\n        title (str): Title of graph\n        x_head (str): Header name for x column data\n        y_head (str): Header name for y column data\n        y_label (str): y axis label\n        ts_df (dataframe): Time series dataframe\n        offsets (list): list of adjacent values to consider\n        weights (list): list of weights applied to offsets\n\n    \"\"\"\n\n    avg = ts_df.withColumn(\"avg\", weighted_average(ts_df, offsets, weights)).drop(y_head)\n    avg = avg.select(avg[x_head],\n                     avg[\"avg\"].alias(y_head))\n\n    plot_ts(title, y_label, [avg])\n    \n    "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Load data into the notebook\ndf_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\n\n# Rename and cast types for each column\ndf_01 = df_01.select(\n      df_01[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_01[\"name\"].alias(\"tag\"),\n      df_01[\"value\"]\n)\n\ndf_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\n\ndf_02 = df_02.select(\n      df_02[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_02[\"name\"].alias(\"tag\"),\n      df_02[\"value\"]\n)\n\n#display(df_02.select(\"tag\").distinct())"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["explore = GroupDataTools(df_01)\nexplore.append_data(df_02)\nexplore.is_null(explore.df)\nexplore.df = explore.null2zero(\"value\", explore.df)\nexplore.is_null(explore.df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["explore.groupdata(\"tag\", \"datetime\", \"value\")\n\nr1 = re.compile('BRA-....-..-07.')\nr2 = re.compile('BRA-QT  -15-0077-RAW')\nOW1 = explore.splitdata_dict([r1, r2])\n\nr1 = re.compile('BRA-....-..-01.')\nr2 = re.compile('BRA-QT  -15-0017-RAW')\nOW3 = explore.splitdata_dict([r1, r2])\n\nr1 = re.compile('BRA-....-..-04.')\nOW2 = explore.splitdata_dict([r1])\n\n# Make a tag dictionary: Decode the tags!\ntag_names = {\n              \"BRA-PZT........\" : \"WHP\",\n              \"BRA-TT..-15....\" : \"WHT\",\n              \"BRA-FI.........\" : \"GLR\",\n              \"BRA-PT..-16....\" : \"GLP\",\n              \"BRA-PT..-13....\" : \"DHP\",\n              \"BRA-TT..-13....\" : \"DHT\",\n              \"BRA-HV.........\" : \"Choke\",\n              \"BRA-QT.........\" : \"ASD\"\n}\n\nOW1 = explore.decode_keys(OW1, tag_names)\nOW2 = explore.decode_keys(OW2, tag_names)\nOW3 = explore.decode_keys(OW3, tag_names)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"DHT\"], OW1[\"WHT\"], OW1[\"GLP\"]]\nts_labels = [\"DHP\", \"WHP\", \"DHT\", \"WHT\", \"GLP\"]\n\nge2016 = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHP, WHT, DHP, DHT, GLP over time\", \"datetime\", \"value\", ge2016, ts_labels)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"GLP\"]]\nts_labels = [\"DHP\", \"WHP\", \"GLP\"]\n\nge2016p = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHP, DHP, GLP over time\", \"datetime\", \"value\", ge2016p, ts_labels)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHT\"], OW1[\"WHT\"]]\nts_labels = [\"DHT\", \"WHT\"]\n\nge2016t = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHT, DHT over time\", \"datetime\", \"value\", ge2016t, ts_labels)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Load new data containing rates into the notebook\nOW1_ql_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\n\n# Add the relevant columns to the oilwell dictionary\nOW1[\"LR\"] = OW1_ql_df.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    OW1_ql_df[\"Daily liquid rate [Sm3/d]\"].alias(\"value\")\n)\n\nOW1[\"OR\"] = OW1_ql_df.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    OW1_ql_df[\"Daily oil [Sm3/d]\"].alias(\"value\")\n)\n\n# - Compare if WHP, DHP, GLR are the same as the dump ones!"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"GLP\"], OW1[\"OR\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"GLP\", \"OR\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Normalised WHP, DHP, GLP, OR, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Normalised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\")) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Standardised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Import in the data:\ndf_records = spark.read.format('csv').options(header='true', inferSchema='true', delimiter='|', encoding='iso-8859-1').load('/FileStore/tables/interences_filtered.csv')\n\n# Translate column to english!\ndf_records, trans_dict = explore.translate_col(\"Description\", df_records)\n\n# Pick out only date and description columns\nOW1[\"Records\"] = df_records.select(\n                 F.to_timestamp(F.col(\"Date\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                 df_records[\"Description\"]\n)\n\n# recorded interfaces begin 2017-01-20\nOW1[\"WHP_2017_2019\"] = OW1[\"WHP\"].where(OW1[\"WHP\"].datetime >= '2017-01-20')\n\nabrv_dict = {\n            \"due to sand\" : \"Choking due to sand\",\n            \"Valve test\" : \"Valve test\"\n}\n\n# Overlay time series graph with the records\nOW1[\"WHP_2017_2019\"] = explore.ts_overlay_records(OW1[\"WHP_2017_2019\"], OW1[\"Records\"], \"Description\", filt_dict=abrv_dict, translate_dict=trans_dict)\n\n# Remove any duplicates, and merge any related comments\nOW1[\"WHP_2017_2019\"] = explore.merge_duplicate(OW1[\"WHP_2017_2019\"])\n\n# Produce a discretisation of the columns\nOW1[\"WHP_2017_2019\"] = explore.discretise_col(OW1[\"WHP_2017_2019\"], \"Grouped\")\n\ndisplay(OW1[\"WHP_2017_2019\"])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Add a year column\nOW1[\"WHP_2017_2019\"] = explore.add_year_col(OW1[\"WHP_2017_2019\"])\n\n# Seperate dataframe by year\nyears = ['2017', '2018', '2019']\ndfs_years = [OW1[\"WHP_2017_2019\"].where(OW1[\"WHP_2017_2019\"].year == y).toPandas() for y in years]\n\nfig, axs = plt.subplots(3, 1, figsize=(24, 24))\naxs.flatten()\n\nfor df, ax, year in zip(dfs_years, axs, years):\n  groups = df.groupby('Grouped')\n  ax.plot(df.datetime, df.value, marker='', linestyle='-')\n  for name, group in groups:\n    if name == \"No Records\": continue\n    ax.plot(group.datetime, group.value, marker='o', linestyle='', label=name)\n  ax.legend(loc=\"best\")\n  ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n  ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n  ax.set_title(year, fontsize=16)\n  ax.set_xlabel(\"datetime\", fontsize=16)\n  ax.set_ylabel(\"value\", fontsize=16)\n  ax.grid(True)\n\nfig.tight_layout(pad=0.4, w_pad=0.5, h_pad=3.0)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(OW1[\"WHP_2017_2019\"])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["offsets =  [i for i in range(24)] \nweights = [24 for i in range(24)]\nprint(len(offsets))\nprint(len(weights))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["OW1[\"WHP_2017_2019\"] = OW1[\"WHP_2017_2019\"].withColumn(\"day\", F.date_trunc(\"day\", OW1[\"WHP_2017_2019\"].datetime))\n\ndf_OW1 = OW1[\"WHP_2017_2019\"]\\\n              .groupBy(\"day\")\\\n              .agg(F.avg(\"value\"))\\\n              .orderBy(\"day\")\n\ndisplay(df_OW1)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df_OW1 = df_OW1.select(df_OW1.day.alias(\"datetime\"), df_OW1[\"avg(value)\"].alias(\"value\"))\n\nts_dfs = [df_OW1]\nts_labels = [\"WHP\"]\n\nexplore.plot_ts(\"WHP daily averaged over time\", \"datetime\", \"value\", ts_dfs, ts_labels)\n# view_moving_avg(\"Moving average of OW1 WHP\", \"WHP (Bar)\", OW1[\"GLR\"], offsets, weights)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["years = ['2017', '2018', '2019']\ndfs_years = [new_df1.where(new_df1.year == y).toPandas() for y in years]\n\nfig, axs = plt.subplots(3,1, figsize=(24, 24))\naxs.flatten()\n\nfor df, ax in zip(dfs_years, axs):\n  groups = df.groupby('grouped_desc')\n  ax.plot(df.datetime, df.value, marker='', linestyle='-')\n  for name, group in groups:\n    ax.plot(group.datetime, group.value, marker='o', linestyle='', label=name)\n    ax.grid(True)\n  ax.legend(loc=\"best\")\n  ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n  ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n  ax.set_title(\"title\", fontsize=16)\n  ax.set_xlabel(\"x_head\", fontsize=16)\n  ax.set_ylabel(\"y_head\", fontsize=16)\n\nfig.tight_layout()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"Data Cleaning and Analysis","notebookId":3265004742220724},"nbformat":4,"nbformat_minor":0}