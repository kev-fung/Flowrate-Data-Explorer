{"cells":[{"cell_type":"code","source":["# Import Modules\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, TimestampType\nfrom pyspark.sql.functions import coalesce, lit, col, lead, lag\nfrom pyspark.sql.functions import stddev, mean, col\nfrom pyspark.sql.window import Window\n\nfrom operator import add\nfrom functools import reduce\n\nfrom googletrans import Translator\n\n# Standard Python Modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mdates\nimport pandas as pd\nimport numpy as np\nimport re"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["class DataframeTools:\n  \"\"\"Parent class for manipulating spark dataframes. \"\"\"\n  \n  def __init__(self, df):\n    self.df = df\n  \n  \n  def update_df(self, new_df):\n    \"\"\"Replace class dataframe with a new one.\n    \n    Args:\n      new_df (dataframe): New dataframe\n      \n    \"\"\"\n    \n    self.df = new_df\n  \n  \n  def append_data(self, new_df):\n    \"\"\"Append another dataframe below the current one. \n    New dataframe must have the same columns as the original dataframe.\n    \n    Args:\n      new_df (dataframe): New dataframe\n      \n    \"\"\"\n    \n    assert self.df.schema == new_df.schema, \"Column headers must match!\"\n    \n    print(\"Current samples: \", self.df.count())\n    print(\"Appending samples: \", new_df.count())\n    \n    self.df = self.df.union(new_df)\n    print(\"Joined samples: \", self.df.count())\n    print(\"\")\n  \n  \n  def is_null(self, dfs):\n    \"\"\"Check all columns in dataframe for null values.\n    \n    Args:\n      dfs (dataframe): Dataframe to be checked on\n      \n    \"\"\"\n    \n    print(\"Number of samples with a null value across columns:\")\n    for col in dfs.schema:\n      head = col.name\n      #print(dfs[head].isNull() == True)\n      print(head, dfs.where(dfs[head].isNull() == True).count())\n    print(\"\")\n    \n    \n  def null2zero(self, head, dfs):\n    \"\"\"Change null values to zero in column\n    \n    Args:\n      head (str): Column name\n      dfs (dataframe): Dataframe given\n      \n    \"\"\"\n    \n    print(\"Replacing null values with zero...\\n\")\n    dfs = dfs.na.fill(0, (head))\n    return dfs\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["class GroupDataTools(DataframeTools):\n  \"\"\"Subclass of Dataframe tools to reorganise dataframes into dictionaries.\n    Tools for visualisation and preprocessing included.\n  \"\"\"\n  \n  def __init__(self, df, df_dict={}):\n    super().__init__(df)\n    self.df_dict = df_dict\n    self.headers = [h.name for h in df.schema]\n    \n    \n  def groupdata(self, dict_group_head, x_head, y_head):\n    \"\"\"Collect rows which contain the same value in dict_group_head column and \n    put them into a dictionary. \n    \n    Keys: distinct value, Value: dataframe whose rows contain distinct value\n    \n    Args:\n      dict_group_head (str): column to split the dataframe\n      x_head (str): first column (normally time) to reconstruct corresponding dataframe\n      y_head (str): second column (normally value) to reconstruct corresponding dataframe\n    \n    \"\"\"\n    \n    # Split dataframe up by given header\n    assert dict_group_head in self.headers, \"Header does not exist in dataframe!\"\n    \n    unq_items = self.df.select(dict_group_head).distinct()\n    n = unq_items.rdd.map(lambda x: x[dict_group_head]).collect()\n\n    for key in n:\n      self.df_dict[key] = self.df.orderBy(x_head).where(self.df[dict_group_head] == key).select(self.df[x_head], self.df[y_head])\n  \n  \n  def splitdata_dict(self, regex):\n    \"\"\"Make a dictionary from df_dict given a conditional substring which matches within the keys of df_dict.\n    E.g. Return a dictionary of a certain oilwell given from a code in the tag name.\n\n    Args:\n      regex (str): substring to match with keys, written in wildcard format for regular expressions\n      \n    \"\"\"\n    \n    out_dict = {}\n    \n    for (key, val) in self.df_dict.items():\n      for reg in regex:\n        if re.match(reg, key):\n          out_dict[key] = val\n      \n    return out_dict\n    \n    \n  def decode_keys(self, in_dict, decode_dict):\n    \"\"\"Replace the old keys with new key definitions.\n\n      Args:\n        in_dict (dict): input dictionary \n        decode_dict (dict): dictionary of old keys with new definitions\n\n      Returns: None\n      \n    \"\"\"\n\n    for key, val in in_dict.items():\n      for k, new_key in decode_dict.items():\n        regex = re.compile(k)\n        if (re.match(regex, key)):\n          print(\"Replacing \", key, \", with \", new_key)\n          in_dict[new_key] = in_dict.pop(key)\n    print(\"\")\n    \n    return None\n  \n  \n  def plot_ts(self, title, x_head, y_head, ts_df_list, label_list=[\"value\"]):\n    \"\"\"Plot multiple timeseries dataframe onto a figure, x axis = time, y axis = value.\n\n    Args:\n      title (str): Name of dataframe\n      x_head (str): Name of column to be plotted along x axis\n      y_head (str): Name of column to be plotted along y axis\n      ts_df_list (list): list of timeseries dataframes to plot\n      label_list (list): list of plot labels\n\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(24, 10))\n\n    for ts_df, lab in zip(ts_df_list, label_list):\n      ts_pd = ts_df.orderBy(x_head).toPandas()\n\n      y = ts_pd[y_head].tolist()\n      x = ts_pd[x_head].tolist()\n\n      ax.plot(x, y, \".--\", label=lab)\n\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel(x_head, fontsize=16)\n    ax.set_ylabel(y_head, fontsize=16)\n\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n    ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n\n    ax.legend(loc=\"best\")\n\n    fig.tight_layout()\n    display(fig)\n    \n\n  def weighted_average(ts_df, x_head, y_head, offsets, weights):\n    \"\"\"Produce rolling average results of the given ts data with the given specs.\n\n      Args:\n        ts_df (dataframe): timeseries dataframe\n        x_head (str): header name of x axis in timeseries (e.g. datetime)\n        y_head (str): header name of y axis in timeseries (e.g. value)\n        offsets (list): list of adjacent values to consider\n        weights (list): list of weights applied to offsets\n\n    \"\"\"\n    window = Window.orderBy(x_head)\n    v = col(y_head)\n\n    assert len(weights) == len(offsets)\n\n    def value(i):\n        if i < 0: return lag(v, -i).over(window)\n        if i > 0: return lead(v, i).over(window)\n        return v\n\n    values = [coalesce(value(i) * w, lit(0))/len(offsets) for i, w in zip(offsets, weights)]\n\n    return reduce(add, values, lit(0))\n  \n  \n  def view_moving_avg(title, x_head, y_head, y_label, ts_df, offsets, weights):\n    \"\"\"Wrapper function to view the moving average of the given ts data\n\n      Args:\n        title (str): Title of graph\n        x_head (str): Header name for x column data\n        y_head (str): Header name for y column data\n        y_label (str): y axis label\n        ts_df (dataframe): Time series dataframe\n        offsets (list): list of adjacent values to consider\n        weights (list): list of weights applied to offsets\n\n    \"\"\"\n\n    avg = ts_df.withColumn(\"avg\", weighted_average(ts_df, offsets, weights)).drop(y_head)\n    avg = avg.select(avg[x_head],\n                     avg[\"avg\"].alias(y_head))\n\n    plot_ts(title, y_label, [avg])\n    \n    "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Load data into the notebook\ndf_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\n\n# Rename and cast types for each column\ndf_01 = df_01.select(\n      df_01[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_01[\"name\"].alias(\"tag\"),\n      df_01[\"value\"]\n)\n\ndf_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\n\ndf_02 = df_02.select(\n      df_02[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_02[\"name\"].alias(\"tag\"),\n      df_02[\"value\"]\n)\n\ndisplay(df_02.select(\"tag\").distinct())"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["explore = GroupDataTools(df_01)\nexplore.append_data(df_02)\nexplore.is_null(explore.df)\nexplore.df = explore.null2zero(\"value\", explore.df)\nexplore.is_null(explore.df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["explore.groupdata(\"tag\", \"datetime\", \"value\")\n\nr1 = re.compile('BRA-....-..-07.')\nr2 = re.compile('BRA-QT  -15-0077-RAW')\nOW1 = explore.splitdata_dict([r1, r2])\n\nr1 = re.compile('BRA-....-..-01.')\nr2 = re.compile('BRA-QT  -15-0017-RAW')\nOW3 = explore.splitdata_dict([r1, r2])\n\nr1 = re.compile('BRA-....-..-04.')\nOW2 = explore.splitdata_dict([r1])\n\n# Make a tag dictionary: Decode the tags!\ntag_names = {\n              \"BRA-PZT........\" : \"WHP\",\n              \"BRA-TT  -15....\" : \"WHT\",\n              \"BRA-FI.........\" : \"GLR\",\n              \"BRA-PT  -16....\" : \"GLP\",\n              \"BRA-PT  -13....\" : \"DHP\",\n              \"BRA-TT  -13....\" : \"DHT\",\n              \"BRA-HV.........\" : \"Choke\",\n              \"BRA-QT.........\" : \"ASD\"\n}\n\nexplore.decode_keys(OW1, tag_names)\nexplore.decode_keys(OW3, tag_names)\nexplore.decode_keys(OW2, tag_names)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"DHT\"], OW1[\"WHT\"], OW1[\"GLP\"]]\nts_labels = [\"DHP\", \"WHP\", \"DHT\", \"WHT\", \"GLP\"]\n\nge2016 = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHP, WHT, DHP, DHT, GLP over time\", \"datetime\", \"value\", ge2016, ts_labels)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"GLP\"]]\nts_labels = [\"DHP\", \"WHP\", \"GLP\"]\n\nge2016p = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHP, DHP, GLP over time\", \"datetime\", \"value\", ge2016p, ts_labels)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHT\"], OW1[\"WHT\"]]\nts_labels = [\"DHT\", \"WHT\"]\n\nge2016t = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n\nexplore.plot_ts(\"WHT, DHT over time\", \"datetime\", \"value\", ge2016t, ts_labels)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Load new data containing rates into the notebook\nOW1_ql_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\n\n# Add the relevant columns to the oilwell dictionary\nOW1[\"LR\"] = OW1_ql_df.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    OW1_ql_df[\"Daily liquid rate [Sm3/d]\"].alias(\"value\")\n)\n\nOW1[\"OR\"] = OW1_ql_df.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    OW1_ql_df[\"Daily oil [Sm3/d]\"].alias(\"value\")\n)\n\n# - Compare if WHP, DHP, GLR are the same as the dump ones!"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"GLP\"], OW1[\"OR\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"GLP\", \"OR\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Normalised WHP, DHP, GLP, OR, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Normalised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\")) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nexplore.plot_ts(\"Standardised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# offsets, weights =  [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n# view_moving_avg(\"Moving average of OW1 WHP\", \"WHP (Bar)\", OW1[\"GLR\"], offsets, weights)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Import in the data:\ninterference_df = spark.read.format('csv').options(header='true', inferSchema='true', delimiter='|', encoding='iso-8859-1').load('/FileStore/tables/interences_filtered.csv')\n\n# Translate descriptions\ntranslator = Translator()\nn = interference_df.select(interference_df[\"Description\"]).distinct().rdd.map(lambda x: x[\"Description\"]).collect()\nno2eng_dict = {desc : translator.translate(desc, src='no', dest='en').text for desc in n}\n\ninterference_df = interference_df.na.replace(no2eng_dict, 1, \"Description\")\n\n# Pick out only date and description columns\nOW1[\"Interf\"] = interference_df.select(\n                 F.to_timestamp(F.col(\"Date\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                 interference_df[\"Description\"]\n)\n\n# def translate_string(sentence):\n#   translator = Translator()\n#   english = translator.translate(sentence, src='no', dest='en').text  \n#   return english\n# udf = F.udf(translate_string, StringType())\n# int_df = interference_df.select(interference_df[\"Date\"], interference_df[\"Description\"])\n# int_df = int_df.withColumn(\"Desc\", udf(\"Description\"))\n# display(int_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df = OW1[\"WHP\"].where(OW1[\"WHP\"].datetime >= '2017-01-11')\n\ndf = df.select(\n  df[\"datetime\"].alias(\"datetime_orig\"),\n  (F.round(F.unix_timestamp(F.col(\"datetime\")) / 86400) * 86400).cast(\"timestamp\").alias(\"datetime\"),\n  df[\"value\"]\n)\n\nnew_df = df.join(OW1[\"Interf\"], on=['datetime'], how='left_outer')\n\n# Convert distinct comments into numbers\nnumerate = { val : str(i) for i, val in enumerate(no2eng_dict.values())}\n\nnew_df = new_df.na.replace(numerate, 1, \"Description\")\nnew_df = new_df.select(\n                new_df[\"datetime_orig\"].alias(\"datetime\"),\n                new_df[\"value\"],\n                new_df[\"Description\"].cast(IntegerType())\n)\n\nnew_df = new_df.na.fill(0, (\"Description\"))\n\nts_pd = new_df.toPandas()\n\n#set pretty params\nsns.set()\n\n\nsns.lmplot('datetime', 'value', data=ts_pd, hue='Description', fit_reg=False)\n\n\n# groups = ts_pd.groupby('Description')\n\n# print(groups)\n\n# # Plot\n# fig, axs = plt.subplots(1, 4, figsize=(24, 10))\n# #axs.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n# axs.flatten()\n\n# for ax in axs:\n#   g\n#   for name, group in groups:\n#       ax.plot(group.datetime, group.value, marker='o', linestyle='', ms=12, label=name)\n\n# ax.grid(True)\n# ax.legend(loc=\"best\")\n# fig.tight_layout()\n\n# display(fig)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\")) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\n\nfig, ax = plt.subplots(1, 1, figsize=(24, 10))\n\nfor ts_df, lab in zip(ge2016norm, ts_labels):\n  ts_pd = ts_df.orderBy(x_head).toPandas()\n\n  y = ts_pd[y_head].tolist()\n  x = ts_pd[x_head].tolist()\n  \n  ax.plot(x, y, label=lab)\n\nax.set_title(title, fontsize=16)\nax.set_xlabel(x_head, fontsize=16)\nax.set_ylabel(y_head, fontsize=16)\n\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\nax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n\nax.legend(loc=\"best\")\n\nfig.tight_layout()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"Data Cleaning and Analysis","notebookId":3265004742220724},"nbformat":4,"nbformat_minor":0}