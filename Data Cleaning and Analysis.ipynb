{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import coalesce, lit, col, lead, lag\n",
    "from pyspark.sql.functions import stddev, mean, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "# Standard Python Modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataframeTools:\n",
    "  \"\"\"Parent class for manipulating spark dataframes. \"\"\"\n",
    "  \n",
    "  def __init__(self, df):\n",
    "    self.df = df\n",
    "    \n",
    "  \n",
    "  def update_df(self, new_df):\n",
    "    \"\"\"Replace class dataframe with a new one.\n",
    "    \n",
    "    Args:\n",
    "      new_df (dataframe): New dataframe\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    self.df = new_df\n",
    "  \n",
    "  \n",
    "  def append_data(self, new_df):\n",
    "    \"\"\"Append another dataframe below the current one. \n",
    "    New dataframe must have the same columns as the original dataframe.\n",
    "    \n",
    "    Args:\n",
    "      new_df (dataframe): New dataframe\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    assert self.df.schema == new_df.schema, \"Column headers must match!\"\n",
    "    \n",
    "    print(\"Current samples: \", self.df.count())\n",
    "    print(\"Appending samples: \", new_df.count())\n",
    "    \n",
    "    self.df = self.df.union(new_df)\n",
    "    print(\"Joined samples: \", self.df.count())\n",
    "    print(\"\")\n",
    "  \n",
    "  \n",
    "  def is_null(self, dfs):\n",
    "    \"\"\"Check all columns in dataframe for null values.\n",
    "    \n",
    "    Args:\n",
    "      dfs (dataframe): Dataframe to be checked on\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Number of samples with a null value across columns:\")\n",
    "    for col in dfs.schema:\n",
    "      head = col.name\n",
    "      #print(dfs[head].isNull() == True)\n",
    "      print(head, dfs.where(dfs[head].isNull() == True).count())\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "  def null2zero(self, head, dfs):\n",
    "    \"\"\"Change null values to zero in column\n",
    "    \n",
    "    Args:\n",
    "      head (str): Column name\n",
    "      dfs (dataframe): Dataframe given\n",
    "    \n",
    "    Returns:\n",
    "      dataframe with replaced null values\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Replacing null values with zero...\\n\")\n",
    "    dfs = dfs.na.fill(0, (head))\n",
    "    return dfs\n",
    "\n",
    "  \n",
    "  def translate_col(self, head, dfs, src='no', dest='en'):\n",
    "    \"\"\"Translate a string column of a dataframe from src language to dest language\n",
    "    \n",
    "    Args:\n",
    "      head (str): Column name\n",
    "      dfs (dataframe): Dataframe given\n",
    "      src (str): Source language to translate from\n",
    "      dest (str): Target language to translate to\n",
    "    \n",
    "    Returns:\n",
    "      new_dfs (dataframe): dataframe with translated column\n",
    "      translation_dict (dict): dictionary of distinct strings translated\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    # Translate descriptions\n",
    "    translator = Translator()\n",
    "    # Select distinct comments\n",
    "    n = dfs.select(dfs[head]).distinct().rdd.map(lambda x: x[head]).collect()\n",
    "    # Make a dictionary to translate distinct comments\n",
    "    translation_dict = {col : translator.translate(col, src=src, dest=dest).text for col in n}\n",
    "    # Utilise spark method and replace all norweigan comments with translated ones\n",
    "    new_dfs = dfs.na.replace(translation_dict, 1, head)\n",
    "    return new_dfs, translation_dict\n",
    "  \n",
    "  \n",
    "  def ts_overlay_records(self, df_ts, df_records, head, filt_dict=None, translate_dict=None):\n",
    "    \"\"\"Given a ts dataframe, make a new column and match corresponding records according to time.\n",
    "       Pass in filt_dict and translate_dict to be able to group up similar records.\n",
    "       \n",
    "    Args:\n",
    "      df_ts (dataframe): Time series dataframe\n",
    "      df_records (dataframe): Time series dataframe whose values contain records (e.g. string descriptions)\n",
    "      head (str): Column name of the records in df_records\n",
    "      filt_dict (dict): Dictionary of similar substrings found in similar records and it's new collective record\n",
    "      translate_dict (dict): Dictionary of translated distinct records\n",
    "      \n",
    "    Returns:\n",
    "      new_df (dataframe): dataframe containing new column of time correlated records to the time series and other useful columns\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Make a new column where datetime precision: daily. (because interferences are recorded daily)\n",
    "    df = df_ts.select(\n",
    "      df_ts[\"datetime\"].alias(\"datetime_orig\"),\n",
    "      (F.round(F.unix_timestamp(F.col(\"datetime\")) / 86400) * 86400).cast(\"timestamp\").alias(\"datetime\"),\n",
    "      df_ts[\"value\"]\n",
    "    )\n",
    "\n",
    "    new_df = df.join(df_records, on=['datetime'], how='left_outer')\n",
    "    \n",
    "    if (filt_dict is not None) and (translate_dict is not None):\n",
    "      print(\"\\nCollecting similar comments given the filt_dict...\")\n",
    "      group_dict = {}\n",
    "\n",
    "      for comment in translate_dict.values():\n",
    "        for abrv, group_comment in filt_dict.items():\n",
    "          if abrv in comment:\n",
    "            group_dict[comment] = group_comment\n",
    "            #print(comment, \" : \", group_comment)\n",
    "\n",
    "      new_df = new_df.withColumn(\"Grouped\", new_df[head])\n",
    "      new_df = new_df.na.replace(group_dict, 1, \"Grouped\")\n",
    "      new_df = new_df.na.fill(\"No Records\", \"Grouped\")\n",
    "    \n",
    "    new_df = new_df.na.fill(\"No Records\", head)\n",
    "    \n",
    "    new_df = new_df.drop(\"datetime\")\n",
    "    new_df = new_df.withColumnRenamed(\"datetime_orig\", \"datetime\")\n",
    "    \n",
    "    print(\"\\nOverlaid records onto timeseries: You may need to remove/merge duplicates!\")\n",
    "    print(\"Duplicates found: \", new_df.dropDuplicates([\"datetime\"]).count())\n",
    "    \n",
    "    return new_df\n",
    "    \n",
    "    \n",
    "  def discretise_col(self, dfs, head):\n",
    "    \"\"\"Make new column for a dataframe which numerically discretises a descriptive column.\n",
    "\n",
    "    Args: \n",
    "      dfs (dataframe): Dataframe which contains: datetime, value, Description, Grouped\n",
    "      head (str): Name of the column to discretise\n",
    "\n",
    "    Returns:\n",
    "      dataframe with column of discretised values of the descriptive column\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Make column by numerically discretising distinct comments\n",
    "    grouped_desc_list = dfs.select(dfs[head]).distinct().rdd.map(lambda x: x[head]).collect()\n",
    "\n",
    "    # Convert comments to discrete values\n",
    "    numerate = {str(val) : str(i) for i, val in enumerate(grouped_desc_list)}\n",
    "\n",
    "    new_dfs = dfs.withColumn(\"Discrete_str\", dfs[head])\n",
    "\n",
    "    new_dfs = new_dfs.na.replace(numerate, 1, \"Discrete_str\")\n",
    "    new_dfs = new_dfs.withColumn(\"Discrete\", new_dfs[\"Discrete_str\"].cast(IntegerType())).drop(\"Discrete_str\")\n",
    "    new_dfs = new_dfs.na.fill(0, (\"Discrete\"))\n",
    "\n",
    "    return new_dfs\n",
    "      \n",
    "      \n",
    "  def add_year_col(self, dfs):\n",
    "    \"\"\"Add column of year of date to dataframe.\n",
    "\n",
    "    Args:\n",
    "      dfs (dataframe): dataframe with datetime\n",
    "\n",
    "    Returns:\n",
    "      dataframe with column for year.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return dfs.withColumn(\"year\", F.year(F.col(\"datetime\")))\n",
    "  \n",
    "  \n",
    "  def merge_duplicate(self, dfs):\n",
    "    \"\"\"Collect up duplicated datetimes with different descriptions, and merge the descriptions together.\n",
    "\n",
    "      Args:\n",
    "        dfs (dataframe): dataframe in the format: datetime|value|description|groupedDescription\n",
    "\n",
    "      Returns:\n",
    "        dataframe with merged descriptions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    reduced = dfs\\\n",
    "        .rdd\\\n",
    "        .map(lambda row: (row[0], [(row[1], row[2], row[3])]))\\\n",
    "        .reduceByKey(lambda x,y: x+y)\\\n",
    "        .map(lambda row: (\n",
    "                row[0],                                #key i.e. datetime\n",
    "                row[1][0][0],    #sum(row[1][0]) / len(row[1][0]),       #value, take the average of the values \n",
    "                ','.join([str(e[1]) for e in row[1]]), #join up the descriptions\n",
    "                ','.join([str(e[2]) for e in row[1]])  #join up the grouped descriptions\n",
    "            )\n",
    "        )\n",
    "\n",
    "    schema_red = dfs.schema\n",
    "    new_dfs = sqlContext.createDataFrame(reduced, schema_red).orderBy(\"datetime\")\n",
    "\n",
    "    if new_dfs.count() > new_dfs.dropDuplicates([\"datetime\"]).count():\n",
    "      raise ValueError('Data has duplicates')\n",
    "\n",
    "    return new_dfs\n",
    "\n",
    "\n",
    "  def avg_over_period(self, dfs, period=\"day\"):\n",
    "    \"\"\"Given a dataframe with datetime column, average over days, weeks, months or years and return new dataframe. \n",
    "      \n",
    "      Args:\n",
    "        dfs (dataframe): dataframe in the format of: datetime|value|...|...\n",
    "      \n",
    "      Returns:\n",
    "        dataframe in format of: datetime|value  , where value is the averaged value over the period\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dfs_new = dfs.withColumn(period, F.date_trunc(period, dfs.datetime))\n",
    "    \n",
    "    dfs_new = dfs_new\\\n",
    "                  .groupBy(period)\\\n",
    "                  .agg(F.avg(\"value\"))\\\n",
    "                  .orderBy(period)\n",
    "    \n",
    "    dfs_new = dfs_new.withColumnRenamed(\"avg(value)\", \"value\")\n",
    "    dfs_new = dfs_new.withColumnRenamed(period, \"datetime\")\n",
    "    \n",
    "    return dfs_new\n",
    "  \n",
    "  \n",
    "  def threshold(self, dfs, thresh=0.7):\n",
    "    \"\"\"Remove any anomalous values based on a threshold cutoff from the mean of the dataset (which has been weekly averaged)\n",
    "    \n",
    "      Args:\n",
    "        dfs (dataframe): input dataframe in format of: datetime|value|...\n",
    "        thresh (double): the percentage range from the mean for thresholding\n",
    "      \n",
    "      Returns:\n",
    "        dataframe with thresholded samples\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    weekly_avg_dfs = self.avg_over_period(dfs, \"week\")\n",
    "    mean, std = weekly_avg_dfs.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n",
    "    \n",
    "    new_dfs = dfs.where((dfs.value >= (1-thresh)*mean) & (dfs.value <= (1+thresh)*mean)).orderBy(\"datetime\")\n",
    "    \n",
    "    remove_count = dfs.select(\"value\").count() - new_dfs.select(\"value\").count()\n",
    "    print(\"\\nThresholding has removed: \", remove_count, \" samples from dataframe\")\n",
    "    \n",
    "    return new_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupDataTools(DataframeTools):\n",
    "  \"\"\"Subclass of Dataframe tools to reorganise dataframes into dictionaries.\n",
    "    Tools for visualisation and preprocessing included.\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, df, df_dict={}):\n",
    "    super().__init__(df)\n",
    "    self.df_dict = df_dict\n",
    "    self.headers = [h.name for h in df.schema]\n",
    "    \n",
    "    \n",
    "  def groupdata(self, dict_group_head, x_head, y_head):\n",
    "    \"\"\"Collect rows which contain the same value in dict_group_head column and \n",
    "    put them into a dictionary. \n",
    "    \n",
    "    Keys: distinct value, Value: dataframe whose rows contain distinct value\n",
    "    \n",
    "    Args:\n",
    "      dict_group_head (str): column to split the dataframe\n",
    "      x_head (str): first column (normally time) to reconstruct corresponding dataframe\n",
    "      y_head (str): second column (normally value) to reconstruct corresponding dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataframe up by given header\n",
    "    assert dict_group_head in self.headers, \"Header does not exist in dataframe!\"\n",
    "    \n",
    "    unq_items = self.df.select(dict_group_head).distinct()\n",
    "    n = unq_items.rdd.map(lambda x: x[dict_group_head]).collect()\n",
    "\n",
    "    for key in n:\n",
    "      self.df_dict[key] = self.df.orderBy(x_head).where(self.df[dict_group_head] == key).select(self.df[x_head], self.df[y_head])\n",
    "  \n",
    "  \n",
    "  def splitdata_dict(self, regex):\n",
    "    \"\"\"Make a dictionary from df_dict given a conditional substring which matches within the keys of df_dict.\n",
    "    E.g. Return a dictionary of a certain oilwell given from a code in the tag name.\n",
    "\n",
    "    Args:\n",
    "      regex (str): substring to match with keys, written in wildcard format for regular expressions\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    out_dict = {}\n",
    "    \n",
    "    for (key, val) in self.df_dict.items():\n",
    "      for reg in regex:\n",
    "        if re.match(reg, key):\n",
    "          out_dict[key] = val\n",
    "      \n",
    "    return out_dict\n",
    "    \n",
    "    \n",
    "  def decode_keys(self, in_dict, decode_dict):\n",
    "    \"\"\"Replace the old keys with new key definitions.\n",
    "\n",
    "      Args:\n",
    "        in_dict (dict): input oilwell dictionary containing dataframes\n",
    "        decode_dict (dict): dictionary of old keys with new definitions\n",
    "\n",
    "      Returns: None\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    new_dict = {}\n",
    "    \n",
    "    for key, val in in_dict.items():\n",
    "      for k, new_key in decode_dict.items():\n",
    "        regex = re.compile(k)\n",
    "        if (re.match(regex, key)):\n",
    "          #print(\"Replacing \", key, \", with \", new_key)\n",
    "          new_dict[new_key] = val\n",
    "          #for i, j in new_dict.items():\n",
    "            #print(i, \" : \", j)\n",
    "    print(\"\")\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "  \n",
    "  def plot_ts(self, title, x_head, y_head, ts_df_list, label_list=[\"value\"], **kwargs):\n",
    "    \"\"\"Plot multiple timeseries dataframe onto a figure, x axis = time, y axis = value.\n",
    "\n",
    "    Args:\n",
    "      title (str): Name of dataframe\n",
    "      x_head (str): Name of column to be plotted along x axis\n",
    "      y_head (str): Name of column to be plotted along y axis\n",
    "      ts_df_list (list): list of timeseries dataframes to plot\n",
    "      label_list (list): list of plot labels\n",
    "      \n",
    "      **kwargs: Additional options:\n",
    "        overlay (str): header of column containing descriptions\n",
    "        overlay_dfs (dataframe): dataframe containing the descriptions,  dataframe format: datetime|value|overlay|...\n",
    "        plot_yearly (list): plot yearly data on separate axes by passing in list of years\n",
    "        plot_quarterly (list): plot quarterly data of given list of years\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if \"plot_yearly\" not in kwargs.keys():\n",
    "      fig, ax = plt.subplots(1, 1, figsize=(24, 8))\n",
    "\n",
    "      for ts_df, lab in zip(ts_df_list, label_list):\n",
    "        ts_pd = ts_df.orderBy(x_head).toPandas()\n",
    "\n",
    "        y = ts_pd[y_head].tolist()\n",
    "        x = ts_pd[x_head].tolist()\n",
    "\n",
    "        ax.plot(x, y, \"-\", label=lab)\n",
    "        ax.grid(True)\n",
    "\n",
    "      if (\"overlay\" in kwargs.keys()) and (\"overlay_dfs\" in kwargs.keys()):\n",
    "        # the overlay dataframe should have a value column which is the same as another column being plotted!\n",
    "        self.__overlay_plot(ax, kwargs[\"overlay\"], kwargs[\"overlay_dfs\"])\n",
    "\n",
    "      ax.set_title(title, fontsize=16)\n",
    "      ax.set_xlabel(x_head, fontsize=16)\n",
    "      ax.set_ylabel(y_head, fontsize=16)\n",
    "\n",
    "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "      ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "\n",
    "      ax.legend(loc=\"best\")\n",
    "\n",
    "      fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=3.0)\n",
    "      display(fig)\n",
    "    \n",
    "    else:\n",
    "      ts_df_years_list = [self.add_year_col(df) for df in ts_df_list]\n",
    "      \n",
    "      # double list: dfs_years[df][year]\n",
    "      # check if input years list is valid in the dfs column of years\n",
    "      dfs_years = [[df.where(df.year == y).toPandas() for y in kwargs[\"plot_yearly\"]] for df in ts_df_years_list]\n",
    "\n",
    "      plots = len(kwargs[\"plot_yearly\"])\n",
    "      \n",
    "      fig, axs = plt.subplots(plots, 1, figsize=(24, 8*plots))\n",
    "      axs.flatten()\n",
    "      \n",
    "      for df, lab in zip(dfs_years, label_list):\n",
    "        for year_plot, ax, year in zip(df, axs, kwargs[\"plot_yearly\"]):\n",
    "          ts_pd = year_plot.sort_values(x_head)\n",
    "          \n",
    "          y = ts_pd[y_head].tolist()\n",
    "          x = ts_pd[x_head].tolist()\n",
    "          \n",
    "          ax.plot(x, y, \"-\", label=lab)\n",
    "          ax.grid(True)\n",
    "          ax.set_title(\"{}, {}\".format(title, year), fontsize=16)\n",
    "          ax.legend(loc=\"best\")\n",
    "          ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "          ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "\n",
    "          ax.set_xlabel(x_head, fontsize=16)\n",
    "          ax.set_ylabel(y_head, fontsize=16)\n",
    "          \n",
    "      if (\"overlay\" in kwargs.keys()) and (\"overlay_dfs\" in kwargs.keys()):\n",
    "        overlay_dfs_ = self.add_year_col(kwargs[\"overlay_dfs\"])\n",
    "        overlay_dfs_years = [overlay_dfs_.where(overlay_dfs_.year == y) for y in years]\n",
    "        \n",
    "        for year_plot, ax in zip(overlay_dfs_years, axs):\n",
    "          self.__overlay_plot(ax, kwargs[\"overlay\"], year_plot)\n",
    "\n",
    "      fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=3.0)\n",
    "      display(fig)\n",
    "      \n",
    "#     if \"plot_quarterly\" in kwargs.keys():\n",
    "#       ts_df_years_list = [self.add_year_col(df) for df in ts_df_list]\n",
    "      \n",
    "#       # double list: dfs_years[df][year]\n",
    "#       # check if input years list is valid in the dfs column of years\n",
    "#       dfs_years = [[df.where(df.year == y).toPandas() for y in kwargs[\"plot_yearly\"]] for df in ts_df_years_list]\n",
    "\n",
    "#       plots = len(kwargs[\"plot_yearly\"])\n",
    "      \n",
    "#       fig, axs = plt.subplots(plots, 1, figsize=(24, 8*plots))\n",
    "#       axs.flatten()\n",
    "      \n",
    "#       for df, lab in zip(dfs_years, label_list):\n",
    "#         for year_plot, ax, year in zip(df, axs, kwargs[\"plot_yearly\"]):\n",
    "#           ts_pd = year_plot.sort_values(x_head)\n",
    "          \n",
    "#           y = ts_pd[y_head].tolist()\n",
    "#           x = ts_pd[x_head].tolist()\n",
    "          \n",
    "#           ax.plot(x, y, \"-\", label=lab)\n",
    "#           ax.grid(True)\n",
    "#           ax.set_title(\"{}, {}\".format(title, year), fontsize=16)\n",
    "#           ax.legend(loc=\"best\")\n",
    "#           ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "#           ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "\n",
    "#           ax.set_xlabel(x_head, fontsize=16)\n",
    "#           ax.set_ylabel(y_head, fontsize=16)\n",
    "          \n",
    "#       if (\"overlay\" in kwargs.keys()) and (\"overlay_dfs\" in kwargs.keys()):\n",
    "#         overlay_dfs_ = self.add_year_col(kwargs[\"overlay_dfs\"])\n",
    "#         overlay_dfs_years = [overlay_dfs_.where(overlay_dfs_.year == y) for y in years]\n",
    "        \n",
    "#         for year_plot, ax in zip(overlay_dfs_years, axs):\n",
    "#           self.__overlay_plot(ax, kwargs[\"overlay\"], year_plot)\n",
    "\n",
    "#       fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=3.0)\n",
    "#       display(fig)\n",
    "  \n",
    "  \n",
    "  def __overlay_plot(self, ax, overlay, overlay_dfs):\n",
    "    dfs_pd = overlay_dfs.toPandas()\n",
    "    groups = dfs_pd.groupby(overlay)\n",
    "\n",
    "    for name, group in groups:\n",
    "      if name == \"No Records\": continue\n",
    "      ax.plot(group.datetime, group.value, marker='o', linestyle='', label=name)\n",
    "      ax.legend(loc=\"best\")\n",
    "  \n",
    "  \n",
    "  def weighted_average(self, ts_df, x_head, y_head, offsets, weights):\n",
    "    \"\"\"Produce rolling average results of the given ts data with the given specs.\n",
    "\n",
    "      Args:\n",
    "        ts_df (dataframe): timeseries dataframe\n",
    "        x_head (str): header name of x axis in timeseries (e.g. datetime)\n",
    "        y_head (str): header name of y axis in timeseries (e.g. value)\n",
    "        offsets (list): list of adjacent values to consider\n",
    "        weights (list): list of weights applied to offsets\n",
    "\n",
    "    \"\"\"\n",
    "    window = Window.orderBy(x_head)\n",
    "    v = col(y_head)\n",
    "\n",
    "    assert len(weights) == len(offsets)\n",
    "\n",
    "    def value(i):\n",
    "        if i < 0: return lag(v, -i).over(window)\n",
    "        if i > 0: return lead(v, i).over(window)\n",
    "        return v\n",
    "\n",
    "    values = [coalesce(value(i) * w, lit(0))/len(offsets) for i, w in zip(offsets, weights)]\n",
    "\n",
    "    return reduce(add, values, lit(0))\n",
    "  \n",
    "  \n",
    "  def view_moving_avg(self, title, x_head, y_head, y_label, ts_df, offsets, weights):\n",
    "    \"\"\"Wrapper function to view the moving average of the given ts data\n",
    "\n",
    "      Args:\n",
    "        title (str): Title of graph\n",
    "        x_head (str): Header name for x column data\n",
    "        y_head (str): Header name for y column data\n",
    "        y_label (str): y axis label\n",
    "        ts_df (dataframe): Time series dataframe\n",
    "        offsets (list): list of adjacent values to consider\n",
    "        weights (list): list of weights applied to offsets\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    avg = ts_df.withColumn(\"avg\", weighted_average(ts_df, offsets, weights)).drop(y_head)\n",
    "    avg = avg.select(avg[x_head],\n",
    "                     avg[\"avg\"].alias(y_head))\n",
    "\n",
    "    plot_ts(title, y_label, [avg])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into the notebook\n",
    "df_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\n",
    "\n",
    "# Rename and cast types for each column\n",
    "df_01 = df_01.select(\n",
    "      df_01[\"Unnamed: 0\"].alias(\"index\"),\n",
    "      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n",
    "      df_01[\"name\"].alias(\"tag\"),\n",
    "      df_01[\"value\"]\n",
    ")\n",
    "\n",
    "df_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\n",
    "\n",
    "df_02 = df_02.select(\n",
    "      df_02[\"Unnamed: 0\"].alias(\"index\"),\n",
    "      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n",
    "      df_02[\"name\"].alias(\"tag\"),\n",
    "      df_02[\"value\"]\n",
    ")\n",
    "\n",
    "#display(df_02.select(\"tag\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = GroupDataTools(df_01)\n",
    "explore.append_data(df_02)\n",
    "explore.is_null(explore.df)\n",
    "explore.df = explore.null2zero(\"value\", explore.df)\n",
    "explore.is_null(explore.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.groupdata(\"tag\", \"datetime\", \"value\")\n",
    "\n",
    "r1 = re.compile('BRA-....-..-07.')\n",
    "r2 = re.compile('BRA-QT  -15-0077-RAW')\n",
    "OW1 = explore.splitdata_dict([r1, r2])\n",
    "\n",
    "r1 = re.compile('BRA-....-..-01.')\n",
    "r2 = re.compile('BRA-QT  -15-0017-RAW')\n",
    "OW3 = explore.splitdata_dict([r1, r2])\n",
    "\n",
    "r1 = re.compile('BRA-....-..-04.')\n",
    "OW2 = explore.splitdata_dict([r1])\n",
    "\n",
    "# Make a tag dictionary: Decode the tags!\n",
    "tag_names = {\n",
    "              \"BRA-PZT........\" : \"WHP\",\n",
    "              \"BRA-TT..-15....\" : \"WHT\",\n",
    "              \"BRA-FI.........\" : \"GLR\",\n",
    "              \"BRA-PT..-16....\" : \"GLP\",\n",
    "              \"BRA-PT..-13....\" : \"DHP\",\n",
    "              \"BRA-TT..-13....\" : \"DHT\",\n",
    "              \"BRA-HV.........\" : \"Choke\",\n",
    "              \"BRA-QT.........\" : \"ASD\"\n",
    "}\n",
    "\n",
    "OW1 = explore.decode_keys(OW1, tag_names)\n",
    "OW2 = explore.decode_keys(OW2, tag_names)\n",
    "OW3 = explore.decode_keys(OW3, tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"DHT\"], OW1[\"WHT\"], OW1[\"GLP\"]]\n",
    "ts_labels = [\"DHP\", \"WHP\", \"DHT\", \"WHT\", \"GLP\"]\n",
    "\n",
    "ge2016 = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n",
    "\n",
    "explore.plot_ts(\"WHP, WHT, DHP, DHT, GLP over time\", \"datetime\", \"value\", ge2016, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"GLP\"]]\n",
    "ts_labels = [\"DHP\", \"WHP\", \"GLP\"]\n",
    "\n",
    "ge2016p = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n",
    "\n",
    "explore.plot_ts(\"WHP, DHP, GLP over time\", \"datetime\", \"value\", ge2016p, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dfs = [OW1[\"DHT\"], OW1[\"WHT\"]]\n",
    "ts_labels = [\"DHT\", \"WHT\"]\n",
    "\n",
    "ge2016t = [df.where(df.datetime >= '2016-01-01') for df in ts_dfs]\n",
    "\n",
    "explore.plot_ts(\"WHT, DHT over time\", \"datetime\", \"value\", ge2016t, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new data containing rates into the notebook\n",
    "OW1_ql_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\n",
    "\n",
    "# Add the relevant columns to the oilwell dictionary\n",
    "OW1[\"LR\"] = OW1_ql_df.select(\n",
    "                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "                                    OW1_ql_df[\"Daily liquid rate [Sm3/d]\"].alias(\"value\")\n",
    ")\n",
    "\n",
    "OW1[\"OR\"] = OW1_ql_df.select(\n",
    "                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "                                    OW1_ql_df[\"Daily oil [Sm3/d]\"].alias(\"value\")\n",
    ")\n",
    "\n",
    "# - Compare if WHP, DHP, GLR are the same as the dump ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the interventions thats occurred over the years\n",
    "ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"GLP\"], OW1[\"OR\"], OW1[\"LR\"]]\n",
    "ts_labels = [\"DHP\", \"WHP\", \"GLP\", \"OR\", \"LR\"]\n",
    "\n",
    "ge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n",
    "\n",
    "for i, df in enumerate(ge2016norm):\n",
    "  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n",
    "  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n",
    "  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n",
    "\n",
    "explore.plot_ts(\"Normalised WHP, DHP, GLP, OR, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the interventions thats occurred over the years\n",
    "ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\n",
    "ts_labels = [\"DHP\", \"WHP\", \"LR\"]\n",
    "\n",
    "ge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n",
    "\n",
    "for i, df in enumerate(ge2016norm):\n",
    "  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n",
    "  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\") - mean) / std)\n",
    "  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n",
    "\n",
    "explore.plot_ts(\"Normalised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the interventions thats occurred over the years\n",
    "ts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"LR\"]]\n",
    "ts_labels = [\"DHP\", \"WHP\", \"LR\"]\n",
    "\n",
    "ge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n",
    "\n",
    "for i, df in enumerate(ge2016norm):\n",
    "  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n",
    "  ge2016norm[i] = df.withColumn(\"value_norm\", (col(\"value\")) / std)\n",
    "  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n",
    "\n",
    "explore.plot_ts(\"Standardised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in the data:\n",
    "df_records = spark.read.format('csv').options(header='true', inferSchema='true', delimiter='|', encoding='iso-8859-1').load('/FileStore/tables/interences_filtered.csv')\n",
    "\n",
    "# Translate column to english!\n",
    "df_records, trans_dict = explore.translate_col(\"Description\", df_records)\n",
    "\n",
    "# Pick out only date and description columns\n",
    "OW1[\"Records\"] = df_records.select(\n",
    "                 F.to_timestamp(F.col(\"Date\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n",
    "                 df_records[\"Description\"]\n",
    ")\n",
    "\n",
    "# recorded interfaces begin 2017-01-20\n",
    "OW1[\"WHP_2017_2019\"] = OW1[\"WHP\"].where(OW1[\"WHP\"].datetime >= '2017-01-20')\n",
    "\n",
    "abrv_dict = {\n",
    "            \"due to sand\" : \"Choking due to sand\",\n",
    "            \"Valve test\" : \"Valve test\"\n",
    "}\n",
    "\n",
    "# Overlay time series graph with the records\n",
    "OW1[\"WHP_2017_2019\"] = explore.ts_overlay_records(OW1[\"WHP_2017_2019\"], OW1[\"Records\"], \"Description\", filt_dict=abrv_dict, translate_dict=trans_dict)\n",
    "\n",
    "# Remove any duplicates, and merge any related comments\n",
    "OW1[\"WHP_2017_2019\"] = explore.merge_duplicate(OW1[\"WHP_2017_2019\"])\n",
    "\n",
    "# Produce a discretisation of the columns\n",
    "OW1[\"WHP_2017_2019\"] = explore.discretise_col(OW1[\"WHP_2017_2019\"], \"Grouped\")\n",
    "\n",
    "display(OW1[\"WHP_2017_2019\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = OW1[\"WHP\"].where(OW1[\"WHP\"].datetime >= '2017-01-20')\n",
    "overlay_df = OW1[\"WHP_2017_2019\"]\n",
    "years = ['2017', '2018', '2019']\n",
    "\n",
    "explore.plot_ts(\"Overlaid records for one parameter WHP\", \"datetime\", \"value\", [ts_df], [\"WHP\"], overlay=\"Grouped\", overlay_dfs=overlay_df, plot_yearly=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_thresh = explore.threshold(OW1[\"WHP_2017_2019\"], 0.8)\n",
    "test_avg = explore.avg_over_period(test_thresh, \"week\")\n",
    "\n",
    "ts_labels = [\"Orig\", \"WHT_thresholded\", \"WHT_thresholded_avg\"]\n",
    "ts_dfs = [OW1[\"WHP_2017_2019\"], test_thresh, test_avg]\n",
    "explore.plot_ts(\"WHT_thresholded_avg\", \"datetime\", \"value\", ts_dfs, ts_labels, overlay=\"Grouped\", overlay_dfs=OW1[\"WHP_2017_2019\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2017', '2018', '2019']\n",
    "explore.plot_ts(\"WHT_thresholded_avg\", \"datetime\", \"value\", ts_dfs, ts_labels, overlay=\"Grouped\", overlay_dfs=OW1[\"WHP_2017_2019\"], plot_yearly=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a year column\n",
    "OW1[\"WHP_2017_2019\"] = explore.add_year_col(OW1[\"WHP_2017_2019\"])\n",
    "\n",
    "# Seperate dataframe by year\n",
    "years = ['2017', '2018', '2019']\n",
    "dfs_years = [OW1[\"WHP_2017_2019\"].where(OW1[\"WHP_2017_2019\"].year == y).toPandas() for y in years]\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(24, 24))\n",
    "axs.flatten()\n",
    "\n",
    "for df, ax, year in zip(dfs_years, axs, years):\n",
    "  groups = df.groupby('Grouped')\n",
    "  ax.plot(df.datetime, df.value, marker='', linestyle='-')\n",
    "  for name, group in groups:\n",
    "    if name == \"No Records\": continue\n",
    "    ax.plot(group.datetime, group.value, marker='o', linestyle='', label=name)\n",
    "  ax.legend(loc=\"best\")\n",
    "  ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "  ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "  ax.set_title(year, fontsize=16)\n",
    "  ax.set_xlabel(\"datetime\", fontsize=16)\n",
    "  ax.set_ylabel(\"value\", fontsize=16)\n",
    "  ax.grid(True)\n",
    "  \n",
    "\n",
    "fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=3.0)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets =  [i for i in range(24)] \n",
    "weights = [24 for i in range(24)]\n",
    "print(len(offsets))\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OW1[\"WHP_2017_2019\"] = OW1[\"WHP_2017_2019\"].withColumn(\"day\", F.date_trunc(\"day\", OW1[\"WHP_2017_2019\"].datetime))\n",
    "\n",
    "# df_OW1 = OW1[\"WHP_2017_2019\"]\\\n",
    "#               .groupBy(\"day\")\\\n",
    "#               .agg(F.avg(\"value\"))\\\n",
    "#               .orderBy(\"day\")\n",
    "\n",
    "# display(df_OW1)\n",
    "\n",
    "\n",
    "test_avg = explore.avg_over_period(OW1[\"WHP_2017_2019\"], \"week\")\n",
    "ts_dfs = [test_avg]\n",
    "ts_labels = [\"WHT_averaged_by_week\"]\n",
    "explore.plot_ts(\"WHT_averaged_by_week\", \"datetime\", \"value\", ts_dfs, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dfs = [OW1[\"WHP_2017_2019\"].select(\"datetime\", \"value\")]\n",
    "ts_labels = [\"WHT\"]\n",
    "explore.plot_ts(\"WHT\", \"datetime\", \"value\", ts_dfs, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OW1 = df_OW1.select(df_OW1.day.alias(\"datetime\"), df_OW1[\"avg(value)\"].alias(\"value\"))\n",
    "\n",
    "ts_dfs = [df_OW1]\n",
    "ts_labels = [\"WHP\"]\n",
    "\n",
    "explore.plot_ts(\"WHP daily averaged over time\", \"datetime\", \"value\", ts_dfs, ts_labels)\n",
    "# view_moving_avg(\"Moving average of OW1 WHP\", \"WHP (Bar)\", OW1[\"GLR\"], offsets, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_thresh = explore.threshold(OW1[\"WHP_2017_2019\"], 0.8)\n",
    "test_avg = explore.avg_over_period(test_thresh, \"week\")\n",
    "\n",
    "ts_labels = [\"Orig\", \"WHT_thresholded\", \"WHT_thresholded_avg\"]\n",
    "ts_dfs = [OW1[\"WHP_2017_2019\"], test_thresh, test_avg]\n",
    "explore.plot_ts(\"WHT_thresholded_avg\", \"datetime\", \"value\", ts_dfs, ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "Data Cleaning and Analysis",
  "notebookId": 3.265004742220724E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
