{"cells":[{"cell_type":"markdown","source":["## Oilfield Scale Detection \n  Kevin Fung\n\n---\n\nFollowing notebook is split into 3 main sections for investigating oilwell scale prediction:\n\n### \n1. **Workflow Preparation**: Set up github communication, import modules, perform unit testing\n2. **Data Processing**: Import relevant data, preprocess data, organise data for analysis\n3. **Data Exploration**: Visualisation of relevant data, for gaining insight into data\n4. **Data Analysis & Modelling**: Predition modelling of data, and investigations into data"],"metadata":{}},{"cell_type":"code","source":["%sh \n\ncwd=$(pwd)\ndoes_my_git_repo_exist=\"$cwd/acse-9-independent-research-project-kkf18\"\n\nif [ -d $does_my_git_repo_exist ] \nthen\n  echo \"Git repo exists!\"\n  echo \"Pulling master branch...\"\n  cd acse-9-independent-research-project-kkf18\n  git pull origin master\nelse\n  git clone https://github.com/msc-acse/acse-9-independent-research-project-kkf18.git\n  echo \"Git repo does not exist!\"\n  echo \"Cloning repo...\"\nfi\n\ncd acse-9-independent-research-project-kkf18\ngit branch -a\ngit checkout Prediction_Modelling\ngit pull origin Prediction_Modelling"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Git repo exists!\nPulling master branch...\nFrom https://github.com/msc-acse/acse-9-independent-research-project-kkf18\n * branch            master     -&gt; FETCH_HEAD\nAlready up-to-date.\n/bin/bash: line 15: cd: acse-9-independent-research-project-kkf18: No such file or directory\n* Prediction_Modelling\n  master\n  remotes/origin/Exploratory_Data_Analysis\n  remotes/origin/HEAD -&gt; origin/master\n  remotes/origin/Prediction_Modelling\n  remotes/origin/master\nAlready on &#39;Prediction_Modelling&#39;\nYour branch is up-to-date with &#39;origin/Prediction_Modelling&#39;.\nFrom https://github.com/msc-acse/acse-9-independent-research-project-kkf18\n * branch            Prediction_Modelling -&gt; FETCH_HEAD\nAlready up-to-date.\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["import os\nimport sys\nimport importlib.util\nfrom pathlib import Path\n\n# Add repo path to our sys.path for importing modules from repo.\ndef import_mod(module_name):\n  cwd = os.getcwd()\n  my_git_repo_exists = Path('{}/acse-9-independent-research-project-kkf18'.format(cwd))\n\n  spec = importlib.util.spec_from_file_location(\"{}.py\".format(module_name), \"{}/{}.py\".format(my_git_repo_exists, module_name))\n  module = importlib.util.module_from_spec(spec)\n  spec.loader.exec_module(module)\n  \n  # load module into the sys module dictionary so it can be imported in\n  sys.modules[module_name] = module\n  \n  print(\"Import successful\")\n  \n  assert module_name in sys.modules.keys()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import * \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\n\n# Homemade Modules\nimport_mod(\"Data_Catalogue\")\nimport_mod(\"Data_Process\")\nimport_mod(\"Data_Analytics\")\n\nimport Data_Catalogue as catalogue\nimport Data_Process as process\nimport Data_Analytics as model\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Import successful\nImport successful\nImport successful\nImport successful\nImport successful\nImport successful\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["cwd = os.getcwd()\nmy_git_repo_exists = Path('{}/acse-9-independent-research-project-kkf18'.format(cwd))\nsys.path.insert(0, my_git_repo_exists)\n\n# Basic Unit Tests for Data Engineering Module\n%run -i \"/databricks/driver/acse-9-independent-research-project-kkf18/test_Data.py\"\n%run -i \"/databricks/driver/acse-9-independent-research-project-kkf18/test_Data_Catalogue.py\"\n%run -i \"/databricks/driver/acse-9-independent-research-project-kkf18/test_Data_Process.py\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Import successful\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 58108), raddr=(&#39;127.0.0.1&#39;, 39117)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 34444), raddr=(&#39;127.0.0.1&#39;, 39489)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 51040), raddr=(&#39;127.0.0.1&#39;, 44499)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 34422), raddr=(&#39;127.0.0.1&#39;, 40313)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 57254), raddr=(&#39;127.0.0.1&#39;, 40957)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 56080), raddr=(&#39;127.0.0.1&#39;, 36533)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 55074), raddr=(&#39;127.0.0.1&#39;, 46317)&gt;\n  self._sock = None\n\nReplacing null values with zero\n\nReplacing null values with zero\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 44506), raddr=(&#39;127.0.0.1&#39;, 46765)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 59906), raddr=(&#39;127.0.0.1&#39;, 36025)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 56122), raddr=(&#39;127.0.0.1&#39;, 35345)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 46446), raddr=(&#39;127.0.0.1&#39;, 35979)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 41082), raddr=(&#39;127.0.0.1&#39;, 42443)&gt;\n  self._sock = None\n-8.074349270001139e-17 1.0\n.\n----------------------------------------------------------------------\nRan 6 tests in 2.868s\n\nOK\nImport successful\nImport successful\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 34148), raddr=(&#39;127.0.0.1&#39;, 43443)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 49806), raddr=(&#39;127.0.0.1&#39;, 39791)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 38632), raddr=(&#39;127.0.0.1&#39;, 46865)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 45504), raddr=(&#39;127.0.0.1&#39;, 37395)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 37264), raddr=(&#39;127.0.0.1&#39;, 34019)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 36024), raddr=(&#39;127.0.0.1&#39;, 41663)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 53932), raddr=(&#39;127.0.0.1&#39;, 44007)&gt;\n  self._sock = None\n\nReplacing null values with zero\n\nReplacing null values with zero\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 41036), raddr=(&#39;127.0.0.1&#39;, 38283)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 45502), raddr=(&#39;127.0.0.1&#39;, 44363)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 42542), raddr=(&#39;127.0.0.1&#39;, 34117)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 59162), raddr=(&#39;127.0.0.1&#39;, 39303)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 54474), raddr=(&#39;127.0.0.1&#39;, 42543)&gt;\n  self._sock = None\n-8.074349270001139e-17 1.0\n.\n----------------------------------------------------------------------\nRan 6 tests in 2.605s\n\nOK\nImport successful\nImport successful\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 56302), raddr=(&#39;127.0.0.1&#39;, 33445)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 55980), raddr=(&#39;127.0.0.1&#39;, 38537)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 54062), raddr=(&#39;127.0.0.1&#39;, 40173)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 34746), raddr=(&#39;127.0.0.1&#39;, 40453)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 41940), raddr=(&#39;127.0.0.1&#39;, 34927)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 50716), raddr=(&#39;127.0.0.1&#39;, 40757)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 37464), raddr=(&#39;127.0.0.1&#39;, 41813)&gt;\n  self._sock = None\n\nReplacing null values with zero\n\nReplacing null values with zero\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 60760), raddr=(&#39;127.0.0.1&#39;, 35329)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 44698), raddr=(&#39;127.0.0.1&#39;, 37021)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 41560), raddr=(&#39;127.0.0.1&#39;, 41511)&gt;\n  self._sock = None\n./databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 50482), raddr=(&#39;127.0.0.1&#39;, 35749)&gt;\n  self._sock = None\n/databricks/python/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=12, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=(&#39;127.0.0.1&#39;, 34434), raddr=(&#39;127.0.0.1&#39;, 39655)&gt;\n  self._sock = None\n-8.074349270001139e-17 1.0\n.\n----------------------------------------------------------------------\nRan 6 tests in 2.656s\n\nOK\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Workbench - Data Processing\n\n---\n1. Import Production and Test Separator data into notebook\n1. Construct oilwell dictionaries containing timeseries production data: `newdump_01`, `newdump_02`, `Qliq`, `interferences_filtered` for OW1, OW2, OW3\n2. Construct oilwell dictionary and dataframe for the test separator data for OW1"],"metadata":{}},{"cell_type":"code","source":["# Load datas into notebook\ndf_01 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_01.csv')\ndf_02 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/newdump_02.csv')\ndf_OW1ql = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/Qliq.csv')\ndf_records = spark.read.format('csv').options(header='true', inferSchema='true', delimiter='|', encoding='iso-8859-1').load('/FileStore/tables/interences_filtered.csv')\n\n# Dumped Data:\n# Rename and cast types for each column\ndf_01 = df_01.select(\n      df_01[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_01[\"name\"].alias(\"tag\"),\n      df_01[\"value\"]\n)\n\ndf_02 = df_02.select(\n      df_02[\"Unnamed: 0\"].alias(\"index\"),\n      F.to_timestamp(F.col(\"ts\").cast(\"string\"), \"dd-MMM-yy HH:mm:ss\").alias(\"datetime\"),\n      df_02[\"name\"].alias(\"tag\"),\n      df_02[\"value\"]\n)\n\n# Organise messy time series into sorted dictionary\norganise = catalogue.DictionaryTools()\ndf = organise.append_data(df_01, df_02)\ndf = organise.removenulls(\"value\", df)\n\n# Tag Simplification\ndf_dict = organise.separate2dict(df, \"tag\", \"datetime\", \"value\")\n\nr1 = re.compile('BRA-....-..-07.')\nr2 = re.compile('BRA-QT  -15-0077-RAW')\nOW1 = organise.separate2dict_substr(df_dict, [r1, r2])\n\nr1 = re.compile('BRA-....-..-01.')\nr2 = re.compile('BRA-QT  -15-0017-RAW')\nOW3 = organise.separate2dict_substr(df_dict, [r1, r2])\n\nr1 = re.compile('BRA-....-..-04.')\nOW2 = organise.separate2dict_substr(df_dict, [r1])\n\n# Make a tag dictionary: Decode the tags!\ntag_names = {\n              \"BRA-PZT........\" : \"WHP\",\n              \"BRA-TT..-15....\" : \"WHT\",\n              \"BRA-FI.........\" : \"GLR\",\n              \"BRA-PT..-16....\" : \"GLP\",\n              \"BRA-PT..-13....\" : \"DHP\",\n              \"BRA-TT..-13....\" : \"DHT\",\n              \"BRA-HV.........\" : \"Choke\",\n              \"BRA-QT.........\" : \"ASD\"\n}\n\nOW1 = organise.decode_keys(OW1, tag_names)\nOW2 = organise.decode_keys(OW2, tag_names)\nOW3 = organise.decode_keys(OW3, tag_names)\n\n\n# MIKON Liquid Rate Data:\nOW1[\"MIKON Qliq\"] = df_OW1ql.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    df_OW1ql[\"Daily liquid rate [Sm3/d]\"].alias(\"value\")\n)\n\nOW1[\"MIKON Qoil\"] = df_OW1ql.select(\n                                   F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                                    df_OW1ql[\"Daily oil [Sm3/d]\"].alias(\"value\")\n)\n\n\n# Interference Records Data:\n# Translate records from norwegian to english\ndf_records, trans_dict = process.translate_col(\"Description\", df_records)\n\n# Pick out only date and description columns\nOW1[\"Records\"] = df_records.select(\n                 F.to_timestamp(F.col(\"Date\").cast(\"string\"), \"MM/dd/yyyy\").alias(\"datetime\"),\n                 df_records[\"Description\"]\n)\n\n\n# EXAMPLE USE CASE\n# Make a dataframe with a column of matching records for timeseries data\n# Recorded interferences begin 2017-01-20\nOW1[\"WHP_2017_2019\"] = OW1[\"WHP\"].where(OW1[\"WHP\"].datetime >= '2017-01-20')\n\nabrv_dict = {\n            \"due to sand\" : \"Choking due to sand\",\n            \"Valve test\" : \"Valve test\"\n}\n\n# Match the records to the timeseries data\nOW1[\"WHP_2017_2019\"] = process.ts_overlay_records(OW1[\"WHP_2017_2019\"], OW1[\"Records\"], \"Description\", filt_dict=abrv_dict, translate_dict=trans_dict)\n\n# Merge any two different records with matching timestamps into one record\nOW1[\"WHP_2017_2019\"] = process.merge_duplicate(OW1[\"WHP_2017_2019\"], sqlContext)\n\n# Add column which numerates the distinct records\nOW1[\"WHP_2017_2019\"] = process.numerate_desc(OW1[\"WHP_2017_2019\"], \"Grouped\")\n\n\n# Test separator data\ntest_separator_data = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/testseparator_1.csv')\ntest_OW1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/OW1test_1.csv')\n\ntest_OW1 = test_OW1.select(F.to_timestamp(F.col(\"DATE\").cast(\"string\"), \"MM/dd/yyyy HH:mm\").alias(\"datetime\"), *[n for n in test_OW1.schema.names if not n == \"DATE\"])\ntest_OW1 = test_OW1.where(test_OW1.WHT != \"-\")\ntest_OW1 = test_OW1.withColumn(\"WHT_\", test_OW1[\"WHT\"].cast(\"double\"))\ntest_OW1 = test_OW1.drop(\"WHT\").withColumnRenamed(\"WHT_\", \"WHT\")\ntest_OW1_dict = organise.df2dict(test_OW1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nCurrent samples:  865484\nAppending samples:  601804\nJoined samples:  1467288\n/databricks/spark/python/pyspark/sql/dataframe.py:1818: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n  warnings.warn(&#34;to_replace is a dict and value is not None. value will be ignored.&#34;)\n\nCollecting similar comments given the filt_dict...\n/databricks/spark/python/pyspark/sql/dataframe.py:1818: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n  warnings.warn(&#34;to_replace is a dict and value is not None. value will be ignored.&#34;)\n\nOverlaid records onto timeseries: You may need to remove/merge duplicates!\n\nDuplicates found:  20669\n\nDuplicates have been merged\n/databricks/spark/python/pyspark/sql/dataframe.py:1818: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n  warnings.warn(&#34;to_replace is a dict and value is not None. value will be ignored.&#34;)\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Workbench - Data Exploration\n\n---\n1. Test if plot functionality works"],"metadata":{}},{"cell_type":"code","source":["# Try to find the interventions thats occurred over the years\nts_dfs = [OW1[\"DHP\"], OW1[\"WHP\"], OW1[\"MIKON Qliq\"]]\nts_labels = [\"DHP\", \"WHP\", \"LR\"]\n\nge2016norm = [df.where(df.datetime >= '2016-01-11') for df in ts_dfs]\n\nfor i, df in enumerate(ge2016norm):\n  mean, std = df.select(F.mean(\"value\"), F.stddev(\"value\")).first()\n  ge2016norm[i] = df.withColumn(\"value_norm\", (F.col(\"value\")) / std)\n  ge2016norm[i] = ge2016norm[i].select(ge2016norm[i][\"datetime\"], ge2016norm[i][\"value_norm\"].alias(\"value\"))\n\nfig, axs = process.plot_ts(\"Standardised WHP, DHP, LR over time\", \"datetime\", \"value\", ge2016norm, ts_labels)\ndisplay(fig)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/470bf32c-bd1a-4e4e-a353-c7cfe2161937.png"]}}],"execution_count":9},{"cell_type":"markdown","source":["## Workbench - Data Analysis & Modelling\n\n---"],"metadata":{}},{"cell_type":"code","source":["display(model.correlate(test_OW1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["/plots/d5794fd9-6ea7-4db2-985a-85de277fa20a.png"]}}],"execution_count":11},{"cell_type":"code","source":["# Lets set up a benchmark model with Linear Regression using Test Separator Data\n# Note: these features are selected based on what is available both in the test separator and production data\nfeatures = [\"WHP\", \"WHT\", \"Choke\", \"Qgl\", \"pbh\"]\n\nbm_dataset = model.ProcessDatasets(test_OW1)\n\nwhen = lambda df : df.where((df[\"datetime\"] >= \"2018-06-01\") & (df[\"datetime\"] <= \"2020-01-01\"))\nlabel_col = lambda df : df.withColumn(\"label\", df[\"Q liq\"])\ndrop_nulls = lambda df : df.select(*([\"datetime\"] + features + [\"label\"])).na.drop()\n\nbm_dataset.apply_transformations([when, label_col, drop_nulls])\nbm_X, bm_y = bm_dataset.split_df(features)\nbm_y = np.squeeze(bm_y)\n\n# Train model on the entire test separator dataset\nbm_model = LinearRegression(fit_intercept=True)\nbm_model.fit(bm_X, bm_y)\n\nprint(\"\\nFitted Parameters: \")\nfor i, f in enumerate(features):\n  print(\"{} : {}\".format(f, bm_model.coef_[i]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nReset dataframe to original\nDataframe has been transformed\nDataframe has been split and returned\n\nFitted Parameters: \nWHP : -6.071049469223777\nWHT : 11.992089509625941\nChoke : -5.600466922098723\nQgl : 0.0005726459415793528\npbh : -16.936937001621114\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Evaluate performance of Linear Regression using LOOCV\nlinreg = LinearRegression(fit_intercept=True)\nrandfor = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=1000)\nprint(\"Linear Regression\")\nmodel.kfold_scores(linreg, bm_X, np.squeeze(bm_y), k=len(bm_y))\nprint(\"\\nRandom Forests\")\nmodel.kfold_scores(randfor, bm_X, np.squeeze(bm_y), k=len(bm_y))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Linear Regression\n\nModel Evaluation\n-----------------\nNumber of folds:  32\nAveraged AIC:  -6.354768504899664\nAveraged RMS:  146.79284622508985\n\nRandom Forests\n\nModel Evaluation\n-----------------\nNumber of folds:  32\nAveraged AIC:  -5.713823642219003\nAveraged RMS:  15.011937788046936\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"Workspace","notebookId":113678418321369},"nbformat":4,"nbformat_minor":0}